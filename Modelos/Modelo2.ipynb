{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Verifica si CUDA (GPU) está disponible\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA está disponible. GPU detectada:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA no está disponible. Se usará la CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1,2 y 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando la copia de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACIÓN DE RUTAS, DATASETS Y CLASES COMBINADAS\n",
    "\n",
    "base_path = r\"C:/Users/ALEX/OneDrive/Cursos/Maestría en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/Datasets\"\n",
    "\n",
    "# Definición de cada dataset con su ruta, lista de nombres y mapeo a la lista combinada.\n",
    "# Nota: En D8 se asume que \"Miner\" y \"Rust\" deben fusionarse con \"miner\" y \"rust\" de D1.\n",
    "datasets = {\n",
    "    \"D1\": {\n",
    "        \"path\": os.path.join(base_path, \"D1 - Coffee leaf diseases classification.v5i.yolov11\"),\n",
    "        \"names\": ['cerscospora', 'healthy', 'miner', 'phoma', 'rust'],\n",
    "        \"mapping\": {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}  # idéntico a la lista combinada\n",
    "    },\n",
    "    \"D2\": {\n",
    "        \"path\": os.path.join(base_path, \"D2- Hojas de cafe enfermedades.v2i.yolov11\"),\n",
    "        \"names\": ['Falta-de-boro', 'Falta-de-calcio', 'Falta-de-fosforo', 'Falta-de-hierro', \n",
    "                  'Falta-de-magnesio', 'Falta-de-manganeso', 'Falta-de-potasio', 'Falta-nitrogeno'],\n",
    "        # Se mapea para que los índices de D2 sigan a los de D1 (0 a 4)\n",
    "        \"mapping\": {0: 5, 1: 6, 2: 7, 3: 8, 4: 9, 5: 10, 6: 11, 7: 12}\n",
    "    },\n",
    "    \"D8\": {\n",
    "        \"path\": os.path.join(base_path, \"D8 - Coffee Leaves Detection.v1i.yolov11\"),\n",
    "        \"names\": ['Miner', 'Rust'],\n",
    "        # Se mapea: \"Miner\" se asume igual que \"miner\" (índice 2) y \"Rust\" a \"rust\" (índice 4)\n",
    "        \"mapping\": {0: 2, 1: 4}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lista combinada de nombres (la unión de las clases, en el orden deseado)\n",
    "combined_names = ['cerscospora', 'healthy', 'miner', 'phoma', 'rust',\n",
    "                  'Falta-de-boro', 'Falta-de-calcio', 'Falta-de-fosforo', 'Falta-de-hierro', \n",
    "                  'Falta-de-magnesio', 'Falta-de-manganeso', 'Falta-de-potasio', 'Falta-nitrogeno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR ESTRUCTURA DE CARPETAS COMBINADA\n",
    "\n",
    "# Se creará una única carpeta \"combinado\" con subcarpetas \"train\", \"valid\" y \"test\"\n",
    "combined_path = os.path.join(base_path, \"combinado\")\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    os.makedirs(os.path.join(combined_path, split, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(combined_path, split, \"labels\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path = os.path.normpath(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIAR Y REMAPEAR LOS ARCHIVOS DE CADA DATASET\n",
    "\n",
    "def copy_and_remap_dataset(dataset_name, dataset_info):\n",
    "    \"\"\"\n",
    "    Copia imágenes y etiquetas del dataset original a la estructura combinada.\n",
    "    Renombra los archivos para evitar conflictos y remapea los índices de clase.\n",
    "    \"\"\"\n",
    "    src_path = dataset_info[\"path\"]\n",
    "    mapping = dataset_info[\"mapping\"]\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        src_images = os.path.join(src_path, split, \"images\")\n",
    "        src_labels = os.path.join(src_path, split, \"labels\")\n",
    "        dst_images = os.path.join(combined_path, split, \"images\")\n",
    "        dst_labels = os.path.join(combined_path, split, \"labels\")\n",
    "        \n",
    "        # Copiar imágenes (se renombra con el prefijo del dataset)\n",
    "        if os.path.exists(src_images):\n",
    "            for fname in os.listdir(src_images):\n",
    "                src_file = os.path.join(src_images, fname)\n",
    "                new_fname = f\"{dataset_name}_{fname}\"\n",
    "                dst_file = os.path.join(dst_images, new_fname)\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "        else:\n",
    "            print(f\"Advertencia: No existe la carpeta {src_images}\")\n",
    "        \n",
    "        # Copiar etiquetas y remapear índices\n",
    "        if os.path.exists(src_labels):\n",
    "            for fname in os.listdir(src_labels):\n",
    "                src_file = os.path.join(src_labels, fname)\n",
    "                new_fname = f\"{dataset_name}_{fname}\"\n",
    "                dst_file = os.path.join(dst_labels, new_fname)\n",
    "                with open(src_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                new_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        try:\n",
    "                            cls_old = int(parts[0])\n",
    "                            cls_new = mapping.get(cls_old, cls_old)\n",
    "                            new_line = str(cls_new) + \" \" + \" \".join(parts[1:]) + \"\\n\"\n",
    "                            new_lines.append(new_line)\n",
    "                        except Exception as e:\n",
    "                            new_lines.append(line)\n",
    "                    else:\n",
    "                        new_lines.append(line)\n",
    "                with open(dst_file, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "        else:\n",
    "            print(f\"Advertencia: No existe la carpeta {src_labels}\")\n",
    "\n",
    "# Procesar cada dataset\n",
    "for ds in datasets:\n",
    "    print(f\"Procesando dataset {ds} ...\")\n",
    "    copy_and_remap_dataset(ds, datasets[ds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAR IMÁGENES POR CLASE EN TRAIN (CONJUNTO COMBINADO)\n",
    "\n",
    "def count_images_per_class_combined(combined_path, combined_names):\n",
    "    counts = {i: 0 for i in range(len(combined_names))}\n",
    "    labels_dir = os.path.join(combined_path, \"train\", \"labels\")\n",
    "    for fname in os.listdir(labels_dir):\n",
    "        file_path = os.path.join(labels_dir, fname)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            file_classes = set()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if parts:\n",
    "                    try:\n",
    "                        cls = int(parts[0])\n",
    "                        file_classes.add(cls)\n",
    "                    except:\n",
    "                        continue\n",
    "            for cls in file_classes:\n",
    "                if cls in counts:\n",
    "                    counts[cls] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {file_path}: {e}\")\n",
    "    return counts\n",
    "\n",
    "initial_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "print(\"Conteo inicial de imágenes por clase en TRAIN (combinado):\")\n",
    "for cls, count in initial_counts.items():\n",
    "    print(f\"  Clase {cls} ({combined_names[cls]}): {count} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando el dataaumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION PARA HOMOGENIZAR LAS CLASES EN TRAIN\n",
    "\n",
    "def find_image_file(image_dir, base_name):\n",
    "    \"\"\"\n",
    "    Busca el archivo de imagen en image_dir que coincida con base_name\n",
    "    comprobando las extensiones más comunes.\n",
    "    \"\"\"\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        candidate = os.path.join(image_dir, base_name + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def imread_with_pil(path):\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            return np.array(im.convert(\"RGB\"))\n",
    "    except Exception as e:\n",
    "        print(\"Error al leer con PIL:\", path, e)\n",
    "        return None\n",
    "\n",
    "def augment_image_label(image_path, label_path, out_image_path, out_label_path, angle, brightness_factor):\n",
    "    # Intentar leer con cv2\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        # Si falla, intentar con PIL\n",
    "        img = imread_with_pil(image_path)\n",
    "        if img is None:\n",
    "            print(\"Error al leer la imagen:\", image_path)\n",
    "            return\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Aplicar rotación según el ángulo especificado\n",
    "    if angle != 0:\n",
    "        if angle == 90:\n",
    "            img_aug = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif angle == 180:\n",
    "            img_aug = cv2.rotate(img, cv2.ROTATE_180)\n",
    "        elif angle == 270:\n",
    "            img_aug = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        else:\n",
    "            img_aug = img.copy()\n",
    "    else:\n",
    "        img_aug = img.copy()\n",
    "    \n",
    "    # Ajustar brillo\n",
    "    img_aug = np.clip(img_aug * brightness_factor, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Guardar imagen aumentada\n",
    "    cv2.imwrite(out_image_path, img_aug)\n",
    "    \n",
    "    # Procesar etiqueta: ajustar las coordenadas según la rotación\n",
    "    new_lines = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if parts and len(parts) == 5:\n",
    "                try:\n",
    "                    cls = parts[0]\n",
    "                    x = float(parts[1])\n",
    "                    y = float(parts[2])\n",
    "                    bw = float(parts[3])\n",
    "                    bh = float(parts[4])\n",
    "                    # Ajustar coordenadas para rotaciones en formato YOLO\n",
    "                    if angle == 90:\n",
    "                        new_x = y\n",
    "                        new_y = 1 - x\n",
    "                        new_bw = bh\n",
    "                        new_bh = bw\n",
    "                    elif angle == 180:\n",
    "                        new_x = 1 - x\n",
    "                        new_y = 1 - y\n",
    "                        new_bw = bw\n",
    "                        new_bh = bh\n",
    "                    elif angle == 270:\n",
    "                        new_x = 1 - y\n",
    "                        new_y = x\n",
    "                        new_bw = bh\n",
    "                        new_bh = bw\n",
    "                    else:\n",
    "                        new_x, new_y, new_bw, new_bh = x, y, bw, bh\n",
    "                    new_line = f\"{cls} {new_x:.6f} {new_y:.6f} {new_bw:.6f} {new_bh:.6f}\\n\"\n",
    "                    new_lines.append(new_line)\n",
    "                except Exception as e:\n",
    "                    new_lines.append(line)\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "        with open(out_label_path, 'w') as f:\n",
    "            f.writelines(new_lines)\n",
    "    else:\n",
    "        open(out_label_path, 'w').close()\n",
    "\n",
    "# Definir el objetivo: igualar al máximo número de imágenes por clase en TRAIN\n",
    "current_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "target_count = max(current_counts.values())\n",
    "print(f\"\\nObjetivo de imágenes por clase (TRAIN): {target_count}\")\n",
    "\n",
    "train_images_dir = os.path.join(combined_path, \"train\", \"images\")\n",
    "train_labels_dir = os.path.join(combined_path, \"train\", \"labels\")\n",
    "label_files = os.listdir(train_labels_dir)\n",
    "\n",
    "augmented_counter = 0\n",
    "augmentation_round = 0\n",
    "max_rounds = 8  # para evitar ciclos infinitos\n",
    "\n",
    "while True:\n",
    "    current_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "    min_count = min(current_counts.values())\n",
    "    print(f\"Ronda de augmentación {augmentation_round}: mínimo actual = {min_count}\")\n",
    "    if min_count >= target_count or augmentation_round >= max_rounds:\n",
    "        break\n",
    "    # Recorre cada archivo de etiqueta en TRAIN\n",
    "    for fname in label_files:\n",
    "        label_path = os.path.join(train_labels_dir, fname)\n",
    "        # Leer clases presentes en la imagen\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        if not lines:\n",
    "            continue\n",
    "        image_classes = set()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if parts:\n",
    "                try:\n",
    "                    cls = int(parts[0])\n",
    "                    image_classes.add(cls)\n",
    "                except:\n",
    "                    continue\n",
    "        # Aquí se modifica la condición:\n",
    "        # Solo se aumenta si PARA TODAS las clases de la imagen el conteo actual es menor que target_count.\n",
    "        underrepresented = all(current_counts[cls] < target_count for cls in image_classes)\n",
    "        if underrepresented:\n",
    "            # Se supone que el nombre de la etiqueta y de la imagen son iguales (sin la extensión)\n",
    "            base_name = os.path.splitext(fname)[0]\n",
    "            # Buscar la imagen con extensión correcta\n",
    "            image_path = find_image_file(train_images_dir, base_name)\n",
    "            if image_path is None:\n",
    "                print(\"No se encontró la imagen para:\", base_name)\n",
    "                continue\n",
    "            # Nombres para los archivos aumentados\n",
    "            aug_image_name = base_name + f\"_aug{augmented_counter}\"\n",
    "            aug_label_name = base_name + f\"_aug{augmented_counter}.txt\"\n",
    "            # Mantener la misma extensión que la imagen encontrada\n",
    "            _, ext = os.path.splitext(image_path)\n",
    "            out_image_path = os.path.join(train_images_dir, aug_image_name + ext)\n",
    "            out_label_path = os.path.join(train_labels_dir, aug_label_name)\n",
    "            # Parámetros aleatorios de augmentación\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            brightness_factor = random.uniform(0.7, 1.3)\n",
    "            augment_image_label(image_path, label_path, out_image_path, out_label_path, angle, brightness_factor)\n",
    "            augmented_counter += 1\n",
    "    augmentation_round += 1\n",
    "\n",
    "final_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "print(\"\\nConteo final de imágenes por clase en TRAIN (combinado):\")\n",
    "for cls, count in final_counts.items():\n",
    "    print(f\"  Clase {cls} ({combined_names[cls]}): {count} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "print(\"Conteo final de imágenes por clase en TRAIN (combinado):\")\n",
    "for cls, count in initial_counts.items():\n",
    "    print(f\"  Clase {cls} ({combined_names[cls]}): {count} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAR data.yaml COMBINADO\n",
    "\n",
    "data_yaml = {\n",
    "    \"train\": os.path.join(combined_path, \"train\", \"images\").replace(\"\\\\\", \"/\"),\n",
    "    \"val\": os.path.join(combined_path, \"valid\", \"images\").replace(\"\\\\\", \"/\"),\n",
    "    \"test\": os.path.join(combined_path, \"test\", \"images\").replace(\"\\\\\", \"/\"),\n",
    "    \"nc\": len(combined_names),\n",
    "    \"names\": combined_names\n",
    "}\n",
    "yaml_path = os.path.join(combined_path, \"data.yaml\")\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "print(\"\\ndata.yaml combinado generado en:\", yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import onnxruntime as ort\n",
    "import shutil\n",
    "import subprocess\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación y extracción de métricas\n",
    "def extract_metrics(model):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo usando model.val() y extrae las métricas de validación.\n",
    "    \n",
    "    Si el objeto retornado no es un diccionario, se intenta extraer atributos públicos.\n",
    "    Si no se obtiene una métrica clave (por ejemplo, \"metrics/precision(B)\"), se intenta leer\n",
    "    la última fila del CSV de resultados ubicado en 'runs/detect/train/results.csv'.\n",
    "    \n",
    "    Retorna:\n",
    "      - metrics_dict (dict): Diccionario con las métricas.\n",
    "    \"\"\"\n",
    "    metrics_obj = model.val()\n",
    "    if isinstance(metrics_obj, dict):\n",
    "        metrics_dict = metrics_obj\n",
    "    else:\n",
    "        try:\n",
    "            metrics_dict = metrics_obj.results_dict()\n",
    "        except Exception as e:\n",
    "            print(\"No se pudo usar results_dict(), se intentará extrayendo atributos.\", e)\n",
    "            try:\n",
    "                metrics_dict = {attr: getattr(metrics_obj, attr) \n",
    "                                for attr in dir(metrics_obj)\n",
    "                                if not attr.startswith('_') and not callable(getattr(metrics_obj, attr))}\n",
    "            except Exception as e:\n",
    "                print(\"Error extrayendo atributos del objeto de validación:\", e)\n",
    "                metrics_dict = {}\n",
    "    \n",
    "    # Verificar si se obtuvo la métrica clave; de lo contrario, leer el CSV\n",
    "    if not metrics_dict or \"metrics/precision(B)\" not in metrics_dict:\n",
    "        try:\n",
    "            csv_path = r\"runs\\detect\\train\\results.csv\"\n",
    "            if os.path.exists(csv_path):\n",
    "                results_df = pd.read_csv(csv_path)\n",
    "                # Se toma la última fila, que corresponde a las métricas finales\n",
    "                final_metrics = results_df.iloc[-1].to_dict()\n",
    "                metrics_dict = final_metrics\n",
    "        except Exception as e:\n",
    "            print(\"Error leyendo el CSV de resultados:\", e)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el modelo a ONNX y copiarlo con el nombre deseado\n",
    "def export_and_copy_onnx(model, model_name, imgsz, opset=12):\n",
    "    \"\"\"\n",
    "    Exporta el modelo a ONNX usando model.export(). Por defecto, Ultralytics guarda el ONNX\n",
    "    como 'best.onnx' en 'runs/detect/train/weights'. Esta función espera a que dicho archivo exista,\n",
    "    lo copia a la carpeta 'exports' con el nombre f\"{model_name}.onnx\" y retorna la ruta de destino.\n",
    "    \n",
    "    Se fuerza el parámetro opset para evitar problemas de compatibilidad (por ejemplo, Constant version 19).\n",
    "    \n",
    "    Parámetros:\n",
    "        - model: Instancia del modelo YOLO.\n",
    "        - model_name (str): Nombre identificativo del modelo.\n",
    "        - imgsz (int): Tamaño de la imagen.\n",
    "        - opset (int): Versión del opset para exportar (por ejemplo, 12).\n",
    "    \n",
    "    Retorna:\n",
    "        - onnx_dest_path (str): Ruta al archivo ONNX copiado en la carpeta 'exports'.\n",
    "    \"\"\"\n",
    "    onnx_dest_path = os.path.join(\"exports\", f\"{model_name}.onnx\")\n",
    "    os.makedirs(\"exports\", exist_ok=True)\n",
    "    \n",
    "    # Exporta el modelo con el opset especificado\n",
    "    model.export(format=\"onnx\", imgsz=imgsz, exist_ok=True, opset=opset)\n",
    "    \n",
    "    default_onnx_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.onnx\")\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    # Esperar a que default_onnx_path se cree\n",
    "    while not os.path.exists(default_onnx_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if os.path.exists(default_onnx_path):\n",
    "        shutil.copy2(default_onnx_path, onnx_dest_path)\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el archivo ONNX por defecto en {default_onnx_path}\")\n",
    "    \n",
    "    # Esperar a que el archivo copiado exista en exports\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(onnx_dest_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if not os.path.exists(onnx_dest_path):\n",
    "        print(f\"Error: No se encontró el archivo ONNX en {onnx_dest_path}\")\n",
    "    \n",
    "    return onnx_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el modelo a TensorRT y copiarlo con el nombre deseado\n",
    "def export_and_copy_trt(model, model_name, imgsz, opset=12):\n",
    "    \"\"\"\n",
    "    Exporta el modelo a TensorRT usando model.export(format=\"trt\"). Por defecto, \n",
    "    Ultralytics guarda el engine en 'runs/detect/train/weights' como 'best.trt'. \n",
    "    Esta función espera a que dicho archivo exista, lo copia a la carpeta 'exports' con el nombre\n",
    "    f\"{model_name}.trt\" y retorna la ruta de destino.\n",
    "    \n",
    "    Se fuerza el parámetro opset para la conversión. Si tensorrt no está instalado, se captura la excepción.\n",
    "    \n",
    "    Parámetros:\n",
    "        - model: Instancia del modelo YOLO.\n",
    "        - model_name (str): Nombre identificativo del modelo.\n",
    "        - imgsz (int): Tamaño de la imagen.\n",
    "        - opset (int): Versión del opset para exportar.\n",
    "    \n",
    "    Retorna:\n",
    "        - trt_dest_path (str) o None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.export(format=\"trt\", imgsz=imgsz, exist_ok=True, opset=opset)\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(\"TensorRT no está instalado. Se retornará None para la velocidad TRT.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"Error exportando a TensorRT:\", e)\n",
    "        return None\n",
    "\n",
    "    trt_dest_path = os.path.join(\"exports\", f\"{model_name}.trt\")\n",
    "    os.makedirs(\"exports\", exist_ok=True)\n",
    "    \n",
    "    default_trt_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.trt\")\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(default_trt_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if os.path.exists(default_trt_path):\n",
    "        shutil.copy2(default_trt_path, trt_dest_path)\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el archivo TRT por defecto en {default_trt_path}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(trt_dest_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if not os.path.exists(trt_dest_path):\n",
    "        print(f\"Error: No se encontró el archivo TRT en {trt_dest_path}\")\n",
    "        return None\n",
    "    return trt_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para exportar y copiar el checkpoint en formato PT\n",
    "def export_and_copy_pt(model, model_name):\n",
    "    \"\"\"\n",
    "    Copia el checkpoint del modelo en formato .pt. Por defecto, Ultralytics guarda el checkpoint\n",
    "    como 'best.pt' en 'runs/detect/train/weights'. Esta función espera a que dicho archivo exista,\n",
    "    lo copia a la carpeta 'exports' con el nombre f\"{model_name}.pt\" y retorna la ruta de destino.\n",
    "    \n",
    "    Retorna:\n",
    "      - pt_dest_path (str): Ruta al archivo .pt copiado en la carpeta 'exports'.\n",
    "    \"\"\"\n",
    "    pt_dest_path = os.path.join(\"exports\", f\"{model_name}.pt\")\n",
    "    os.makedirs(\"exports\", exist_ok=True)\n",
    "    \n",
    "    default_pt_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\")\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    # Esperar a que se cree el checkpoint por defecto\n",
    "    while not os.path.exists(default_pt_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if os.path.exists(default_pt_path):\n",
    "        shutil.copy2(default_pt_path, pt_dest_path)\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el archivo PT por defecto en {default_pt_path}\")\n",
    "    \n",
    "    # Esperar a que el archivo copiado exista en exports\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(pt_dest_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if not os.path.exists(pt_dest_path):\n",
    "        print(f\"Error: No se encontró el archivo PT en {pt_dest_path}\")\n",
    "    \n",
    "    return pt_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para medir la velocidad de inferencia en CPU usando ONNX\n",
    "def measure_onnx_inference_speed(onnx_path, imgsz, n_iter=20):\n",
    "    \"\"\"\n",
    "    Mide la velocidad de inferencia en CPU usando onnxruntime.\n",
    "    \n",
    "    Parámetros:\n",
    "      - onnx_path (str): Ruta al archivo ONNX.\n",
    "      - imgsz (int): Tamaño de la imagen (ancho y alto).\n",
    "      - n_iter (int): Número de iteraciones para promediar.\n",
    "    \n",
    "    Retorna:\n",
    "      - Tiempo promedio en milisegundos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ort_session = ort.InferenceSession(onnx_path)\n",
    "        dummy_input = np.random.rand(1, 3, imgsz, imgsz).astype(np.float32)\n",
    "        # Warm-up\n",
    "        for _ in range(5):\n",
    "            _ = ort_session.run(None, {\"images\": dummy_input})\n",
    "        start = time.time()\n",
    "        for _ in range(n_iter):\n",
    "            _ = ort_session.run(None, {\"images\": dummy_input})\n",
    "        end = time.time()\n",
    "        avg_time = (end - start) / n_iter * 1000  # ms\n",
    "        return avg_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error midiendo ONNX runtime: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir la velocidad de inferencia en T4 TensorRT usando trtexec\n",
    "def measure_trt_inference_speed(trt_engine_path, n_iter=20):\n",
    "    \"\"\"\n",
    "    Mide la velocidad de inferencia en una GPU T4 utilizando la herramienta 'trtexec'.\n",
    "    Se asume que 'trtexec' está instalado y en el PATH.\n",
    "    \n",
    "    Parámetros:\n",
    "      - trt_engine_path (str): Ruta al engine de TensorRT (archivo .trt).\n",
    "      - n_iter (int): Número de iteraciones para promediar.\n",
    "    \n",
    "    Retorna:\n",
    "      - Tiempo promedio en milisegundos (float) o None en caso de error.\n",
    "    \"\"\"\n",
    "    if trt_engine_path is None:\n",
    "        return None\n",
    "    try:\n",
    "        cmd = [\"trtexec\", f\"--loadEngine={trt_engine_path}\", f\"--iterations={n_iter}\"]\n",
    "        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "        stdout, stderr = proc.communicate(timeout=120)\n",
    "        avg_time = None\n",
    "        for line in stdout.splitlines():\n",
    "            if \"Average latency\" in line:\n",
    "                parts = line.split(\":\")\n",
    "                if len(parts) > 1:\n",
    "                    avg_time = float(parts[1].strip().split()[0])\n",
    "                    break\n",
    "        if avg_time is None:\n",
    "            print(\"No se pudo extraer el tiempo promedio de la salida de trtexec.\")\n",
    "        return avg_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error midiendo TensorRT runtime: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calular los flops\n",
    "def compute_flops(model, imgsz):\n",
    "    # get_model_complexity_info devuelve un string y el número total de FLOPs\n",
    "    # Asegúrate de que model.model es el objeto PyTorch con el que se pueda contar FLOPs.\n",
    "    macs, params = get_model_complexity_info(model.model, (3, imgsz, imgsz), as_strings=False, print_per_layer_stat=False)\n",
    "    flops = 2 * macs  # MACs a FLOPs (multiplicamos por 2)\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular parámetros y combinar todos los resultados\n",
    "def compute_model_params(model):\n",
    "    \"\"\"\n",
    "    Calcula la cantidad de parámetros del modelo en millones.\n",
    "    \n",
    "    Retorna:\n",
    "      - params (float): Número de parámetros en millones.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.model.parameters()) / 1e6\n",
    "\n",
    "def combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops):\n",
    "    \"\"\"\n",
    "    Combina el nombre del modelo, el tamaño de la imagen, las métricas, la velocidad de inferencia,\n",
    "    la cantidad de parámetros y los FLOPs en un único diccionario.\n",
    "    \n",
    "    Retorna:\n",
    "      - results (dict): Diccionario con toda la información.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"Modelo\": model_name,\n",
    "        \"Tamaño (píxeles)\": imgsz,\n",
    "    }\n",
    "    results.update(metrics_dict)\n",
    "    results.update({\n",
    "        \"Velocidad CPU ONNX (ms)\": avg_time_cpu,\n",
    "        \"Velocidad T4 TensorRT (ms)\": avg_time_trt,\n",
    "        \"Parámetros (M)\": params,\n",
    "        \"FLOPs (B)\": flops\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Fine Tuning y evaluación de modelos\n",
    "\n",
    "# Lista de modelos a fine-tunear. Debes tener disponibles los pesos preentrenados.\n",
    "models_info = [\n",
    "    {\"name\": \"YOLO11n\", \"weights\": \"yolo11n.pt\"},\n",
    "    {\"name\": \"YOLO12n\", \"weights\": \"yolo12n.pt\"},\n",
    "    {\"name\": \"YOLO11s\", \"weights\": \"yolo11s.pt\"},\n",
    "    {\"name\": \"YOLO12s\", \"weights\": \"yolo12s.pt\"}\n",
    "]\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "epochs = 50\n",
    "imgsz = 640\n",
    "batch = 16\n",
    "patience = 5\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Ruta al archivo data.yaml del dataset combinado\n",
    "yaml_path = 'C:/Users/ALEX/OneDrive/Cursos/Maestría en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml'\n",
    "\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el primer modelo: YOLO11n\n",
    "info = models_info[0]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo (fine tuning)\n",
    "train_results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo11n = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo11n)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO12n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el primer modelo: YOLO12n\n",
    "info = models_info[1]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo:\n",
    "train_results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo12n = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo12n)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el primer modelo: YOLO11s\n",
    "info = models_info[2]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo:\n",
    "train_results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo11s = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo11s)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el primer modelo: YOLO12s\n",
    "info = models_info[3]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo:\n",
    "train_results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo12s = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo12s)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisión de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y mostrar la tabla comparativa\n",
    "df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"\\nComparación de modelos fine-tuned:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la tabla en CSV\n",
    "df.to_csv(\"comparacion_modelos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_metrics(df,keywords):\n",
    "    \"\"\"\n",
    "    A partir de un DataFrame con columnas de métricas (por ejemplo, provenientes del CSV de resultados)\n",
    "    detecta cuáles son las métricas más relevantes para comparar modelos de detección y genera un texto\n",
    "    explicativo de la elección.\n",
    "    \n",
    "    Se consideran relevantes las columnas que contengan las palabras clave:\n",
    "      - precision\n",
    "      - recall\n",
    "      - mAP50-95\n",
    "      - mAP50\n",
    "    Adicionalmente, se sugieren opcionalmente columnas como 'epoch' y 'time' para analizar eficiencia.\n",
    "\n",
    "    Retorna:\n",
    "      - relevant_metrics: lista de nombres de columnas relevantes\n",
    "      - optional_metrics: lista de columnas opcionales (como 'epoch' o 'time')\n",
    "      - explanation: texto explicativo de la elección\n",
    "    \"\"\"\n",
    "    # Palabras clave para métricas principales (sin importar mayúsculas o minúsculas)\n",
    "    relevant_metrics = []\n",
    "    for col in df.columns:\n",
    "        for key in keywords:\n",
    "            if key.lower() in col.lower():\n",
    "                relevant_metrics.append(col)\n",
    "                break\n",
    "\n",
    "    # Métricas opcionales que se pueden usar para comparar eficiencia o duración\n",
    "    optional_metrics = []\n",
    "    for opt in ['time', 'epoch']:\n",
    "        if opt in df.columns:\n",
    "            optional_metrics.append(opt)\n",
    "    \n",
    "    explanation = \"Se han seleccionado las siguientes métricas principales para comparar el desempeño de detección:\\n\"\n",
    "    explanation += \", \".join(relevant_metrics) + \".\\n\\n\"\n",
    "    explanation += \"Estas métricas se consideran relevantes porque:\\n\"\n",
    "    explanation += \"- Las métricas con el prefijo 'metrics/' (por ejemplo, \" + \", \".join([m for m in relevant_metrics if 'metrics/' in m]) + \") \" \n",
    "    explanation += \"representan directamente la calidad de la detección (precisión, recall y mAP) en el conjunto de validación.\\n\"\n",
    "    explanation += \"- Los valores de 'mAP50' y 'mAP50-95' permiten evaluar el desempeño en diferentes umbrales de IoU, lo cual es crucial en tareas de detección.\\n\\n\"\n",
    "    explanation += \"Adicionalmente, se consideran opcionales las siguientes columnas para analizar la eficiencia y convergencia:\\n\"\n",
    "    explanation += \", \".join(optional_metrics) + \".\\n\"\n",
    "    explanation += \"Estas columnas indican el tiempo de entrenamiento y el número de epochs, lo que ayuda a comparar la velocidad de convergencia y el costo computacional, aunque no son indicadores directos de la calidad en inferencia.\\n\\n\"\n",
    "    explanation += \"Por otro lado, se omiten columnas con prefijos como 'train/' (pérdidas de entrenamiento) y 'lr/' (tasa de aprendizaje) ya que, aunque son útiles para monitorizar el proceso de optimización, no reflejan directamente el desempeño final en la tarea de detección.\"\n",
    "    \n",
    "    return relevant_metrics, optional_metrics, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['precision', 'recall', 'mAP50-95', 'mAP50']\n",
    "relevant1, optional, text_explanation = select_relevant_metrics(df,keywords)\n",
    "print(text_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Velocidad CPU ONNX (ms)','Parámetros (M)','FLOPs (B)']\n",
    "relevant2, optional, text_explanation = select_relevant_metrics(df,keywords)\n",
    "print(text_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Modelo\",\"epoch\"] + relevant1 + relevant2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_grouped_bars_and_explain(df, relevant_metrics, title=\"Comparación de métricas entre modelos\"):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras agrupadas para las métricas indicadas, comparando cada modelo.\n",
    "    Además, produce un texto explicativo indicando cuál modelo es mejor en cada métrica.\n",
    "\n",
    "    Parámetros:\n",
    "      - df (DataFrame): Debe contener una columna \"Modelo\" y columnas de métricas numéricas.\n",
    "      - relevant_metrics (list): Lista de nombres de columnas de métricas a graficar.\n",
    "      - title (str): Título opcional para el gráfico.\n",
    "\n",
    "    Retorna:\n",
    "      - explanation (str): Un texto que explica cuál modelo es mejor en cada métrica.\n",
    "    \"\"\"\n",
    "    # 1. Subconjunto con \"Modelo\" y las métricas relevantes\n",
    "    subset = df[[\"Modelo\"] + relevant_metrics].copy()\n",
    "\n",
    "    # 2. Establecer \"Modelo\" como índice para facilitar el gráfico\n",
    "    subset.set_index(\"Modelo\", inplace=True)\n",
    "\n",
    "    # 3. Crear gráfico de barras agrupadas\n",
    "    ax = subset.plot(kind=\"bar\", figsize=(10 + 1.5*len(relevant_metrics), 6), rot=0, title=title)\n",
    "    ax.set_ylabel(\"Valor de la métrica\")\n",
    "    # Mover la leyenda para que no tape el gráfico\n",
    "    plt.legend(title=\"Métricas\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Generar explicación sobre los resultados\n",
    "    explanation = f\"En este gráfico se comparan las siguientes métricas: {', '.join(relevant_metrics)}.\\n\"\n",
    "    explanation += \"Cada grupo de barras corresponde a un modelo, y cada color representa una métrica distinta.\\n\\n\"\n",
    "\n",
    "    # 5. Determinar el/los mejor(es) modelo(s) para cada métrica\n",
    "    best_models = {}\n",
    "    for metric in relevant_metrics:\n",
    "        # Buscar el valor máximo\n",
    "        max_value = subset[metric].max()\n",
    "        # Encontrar qué modelo(s) tienen ese valor máximo\n",
    "        best_mods = subset[subset[metric] == max_value].index.tolist()\n",
    "        best_models[metric] = (best_mods, max_value)\n",
    "\n",
    "    # 6. Construir texto explicativo de cuál es mejor en cada métrica\n",
    "    for metric, (best_mods, max_value) in best_models.items():\n",
    "        if len(best_mods) == 1:\n",
    "            explanation += f\"- Para la métrica '{metric}', el mejor modelo es **{best_mods[0]}** con un valor de **{max_value:.4f}**.\\n\"\n",
    "        else:\n",
    "            explanation += f\"- Para la métrica '{metric}', los mejores modelos son **{', '.join(best_mods)}** con un valor de **{max_value:.4f}**.\\n\"\n",
    "\n",
    "    # 7. Resumen: cuántas veces cada modelo fue el mejor\n",
    "    count_best = {}\n",
    "    for metric, (best_mods, _) in best_models.items():\n",
    "        for m in best_mods:\n",
    "            count_best[m] = count_best.get(m, 0) + 1\n",
    "\n",
    "    explanation += \"\\nResumen de cuántas veces cada modelo obtuvo el mejor valor:\\n\"\n",
    "    for m, c in sorted(count_best.items(), key=lambda x: x[1], reverse=True):\n",
    "        explanation += f\"  - {m}: {c} métrica(s)\\n\"\n",
    "\n",
    "    explanation += \"\\nEn base a estas métricas, el modelo con mayor cantidad de 'mejores resultados' se podría considerar superior. \" \\\n",
    "                   \"No obstante, la elección final puede depender de otras consideraciones (velocidad de inferencia, tamaño, etc.).\"\n",
    "\n",
    "    return explanation\n",
    "\n",
    "explanation_text = plot_grouped_bars_and_explain(df, relevant1)\n",
    "print(explanation_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_model_comparison(df, relevant_metrics):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras para cada métrica relevante comparando los modelos.\n",
    "    \n",
    "    Parámetros:\n",
    "      - df: DataFrame que contiene los resultados por modelo.\n",
    "      - relevant_metrics: Lista de columnas (métricas) a graficar.\n",
    "    \"\"\"\n",
    "    num_metrics = len(relevant_metrics)\n",
    "    # Si hay varias métricas, creamos subplots en una fila.\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(5*num_metrics, 5))\n",
    "    \n",
    "    # Si sólo hay una métrica, aseguramos que axes sea una lista.\n",
    "    if num_metrics == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for ax, metric in zip(axes, relevant_metrics):\n",
    "        # Verifica que la columna sea numérica para graficarla\n",
    "        if pd.api.types.is_numeric_dtype(df[metric]):\n",
    "            ax.bar(df[\"Modelo\"], df[metric], color='skyblue')\n",
    "            ax.set_title(metric)\n",
    "            ax.set_xlabel(\"Modelo\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f\"No es numérica: {metric}\", horizontalalignment='center', verticalalignment='center')\n",
    "            ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_model_comparison(df, relevant1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados\n",
    "\n",
    "1. **Métricas principales (precisión, recall, mAP50 y mAP50-95)**  \n",
    "   - **YOLO11s** alcanza la mayor precisión (\\(0.8074\\)), así como los mejores valores de mAP50 (\\(0.8679\\)) y mAP50-95 (\\(0.8167\\)). Esto indica que, en general, YOLO11s tiene la mayor capacidad para detectar correctamente los objetos (alta precisión) y un rendimiento sólido a diferentes umbrales de IoU (mAP).  \n",
    "   - **YOLO11n**, por otro lado, obtiene el mejor recall (\\(0.8399\\)). Esto significa que YOLO11n tiende a detectar un mayor porcentaje de objetos, aunque su precisión sea ligeramente menor.  \n",
    "   - **YOLO12n** y **YOLO12s** no superan a YOLO11s ni a YOLO11n en estas métricas específicas. YOLO12n tiene un recall un poco inferior a YOLO11n y una precisión ligeramente mayor que YOLO11n, pero no alcanza los valores de YOLO11s. Por su parte, YOLO12s queda algo rezagado en todas las métricas consideradas.\n",
    "\n",
    "2. **Tamaño del modelo (número de parámetros)**  \n",
    "   - **YOLO11s** y **YOLO12s** presentan aproximadamente 9.4M y 9.2M parámetros, respectivamente, mientras que **YOLO11n** y **YOLO12n** rondan los 2.5M. Esto implica que los modelos \"s\" (small) son significativamente más grandes que los \"n\" (nano) en términos de capacidad y, potencialmente, de coste computacional.  \n",
    "   - El hecho de que YOLO11s sea el modelo con mejor rendimiento coincide con que sea también uno de los más grandes, lo que sugiere que la mayor capacidad de parámetros podría estar aprovechándose para obtener un mejor ajuste a los datos.\n",
    "\n",
    "3. **Número de épocas (epoch)**  \n",
    "   - YOLO11s entrenó durante 42 épocas, mientras que YOLO12s solo 24. Esto puede indicar que YOLO12s no alcanzó su punto óptimo de entrenamiento. Del mismo modo, YOLO11n y YOLO12n entrenaron 27 y 33 épocas, respectivamente.  \n",
    "   - Podría ser interesante homogeneizar el número de épocas o aplicar técnicas de early stopping consistentes para comparar los modelos en igualdad de condiciones de entrenamiento.\n",
    "\n",
    "4. **Conclusión general**  \n",
    "   - **YOLO11s** es el modelo con mejor rendimiento global en las métricas de precisión, mAP50 y mAP50-95. Sin embargo, es también el más grande en número de parámetros (junto con YOLO12s), lo que implica mayor coste computacional en entrenamiento e inferencia.  \n",
    "   - **YOLO11n** destaca por su alto recall, lo que podría ser valioso en aplicaciones donde es preferible detectar tantos objetos como sea posible (aunque a costa de más falsos positivos). Además, YOLO11n es mucho más ligero que YOLO11s.  \n",
    "   - **YOLO12n** y **YOLO12s** no superan los resultados de sus contrapartes \"11\" en las métricas analizadas, si bien YOLO12n tiene un rendimiento cercano a YOLO11n y también un número de parámetros similar.  \n",
    "   - A la hora de elegir un modelo, es importante balancear la precisión, el recall y el mAP con la complejidad (número de parámetros) y la velocidad de inferencia. En este caso, si la prioridad absoluta es la calidad de detección, YOLO11s es el mejor de los cuatro; si la prioridad es un modelo ligero con alto recall, YOLO11n podría ser la mejor elección.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Cargar el modelo YOLO\n",
    "model_path = \"exports//YOLO11n.pt\"\n",
    "modelo_pytorch = YOLO(model_path)  # Carga la arquitectura + pesos\n",
    "\n",
    "# Definir la carpeta de salida para predicciones\n",
    "pred_dir = r\"C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\Datasets\\predicciones\\comb\"\n",
    "os.makedirs(pred_dir, exist_ok=True)  # Crear carpeta si no existe\n",
    "\n",
    "# Realizar predicciones y guardarlas en la carpeta de destino\n",
    "predictions = modelo_pytorch.predict(\n",
    "    source=r\"C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\",\n",
    "    imgsz=640,\n",
    "    save=True,         # Guarda automáticamente las imágenes con anotaciones\n",
    "    project=pred_dir,  # Guarda los resultados en la carpeta personalizada\n",
    "    name=\"predicciones\"  # Nombre del subdirectorio dentro de `pred_dir`\n",
    ")\n",
    "\n",
    "print(f\"Predicciones guardadas en: {pred_dir}/predicciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación en tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta para el modelo ONNX\n",
    "onnx_path = \"C://Users//ALEX//OneDrive//Cursos//Maestría en Big Data y Data Science//Cursos-VIU//Oblgatorios//TFM//detection-diseases-coffee//exports//YOLO11n.onnx\"\n",
    "\n",
    "# Cargar el modelo ONNX\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# Preparar la representación de TensorFlow\n",
    "tf_rep = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la información de los tensores de salida\n",
    "output_tensor = onnx_model.graph.output[0]\n",
    "output_shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]\n",
    "\n",
    "# La última dimensión suele ser el número de clases\n",
    "num_classes_onnx = output_shape[-1]\n",
    "\n",
    "print(f\"Cantidad de clases en el modelo ONNX: {num_classes_onnx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir_base = \"C://TEMP_MODEL_YOLO11n\" #  Ruta MUY corta y simple\n",
    "saved_model_dir = os.path.join(saved_model_dir_base, \"OUT\") # Subdirectorio también muy corto\n",
    "\n",
    "# Asegurarse de que el directorio base exista \n",
    "os.makedirs(saved_model_dir_base, exist_ok=True) # Asegura que el directorio \"Modelo\" existe\n",
    "# Asegurarse de que el directorio de salida del SavedModel exista\n",
    "os.makedirs(saved_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el SavedModel\n",
    "tf_rep.export_graph(saved_model_dir)\n",
    "print(\"Modelo convertido a TensorFlow SavedModel en:\", saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo usando SavedModel API\n",
    "model = tf.saved_model.load(r\"C:\\TEMP_MODEL_YOLO11n\\OUT\")\n",
    "\n",
    "# Verificar las firmas del modelo\n",
    "print(\"Firmas del modelo:\", list(model.signatures.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la salida del modelo desde la firma \"serving_default\"\n",
    "output_shape = list(model.signatures[\"serving_default\"].structured_outputs.values())[0].shape\n",
    "num_classes = output_shape[-1]  # Última dimensión representa las clases en clasificación\n",
    "\n",
    "print(f\"Cantidad de clases en el modelo SavedModel: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el SavedModel a TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Guardar el modelo TFLite\n",
    "tflite_path = r\"Modelo\\YOLO11n.tflite\"\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"Modelo exportado a TFLite y guardado en:\", tflite_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}