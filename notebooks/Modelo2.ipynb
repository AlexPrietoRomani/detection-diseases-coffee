{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "DATA_DIR = ROOT / 'data'\n",
    "MODEL_DIR = ROOT / 'models'\n",
    "OUTPUT_DIR = ROOT / 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA está disponible. GPU detectada: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verifica si CUDA (GPU) está disponible\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA está disponible. GPU detectada:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA no está disponible. Se usará la CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1,2 y 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando la copia de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACIÓN DE RUTAS, DATASETS Y CLASES COMBINADAS\n",
    "\n",
    "base_path = DATA_DIR / 'raw'\n",
    "\n",
    "# Definición de cada dataset con su ruta, lista de nombres y mapeo a la lista combinada.\n",
    "# Nota: En D8 se asume que \"Miner\" y \"Rust\" deben fusionarse con \"miner\" y \"rust\" de D1.\n",
    "datasets = {\n",
    "    \"D1\": {\n",
    "        \"path\": os.path.join(base_path, \"D1 - Coffee leaf diseases classification.v5i.yolov11\"),\n",
    "        \"names\": ['cerscospora', 'healthy', 'miner', 'phoma', 'rust'],\n",
    "        \"mapping\": {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}  # idéntico a la lista combinada\n",
    "    },\n",
    "    \"D2\": {\n",
    "        \"path\": os.path.join(base_path, \"D2- Hojas de cafe enfermedades.v2i.yolov11\"),\n",
    "        \"names\": ['Falta-de-boro', 'Falta-de-calcio', 'Falta-de-fosforo', 'Falta-de-hierro', \n",
    "                  'Falta-de-magnesio', 'Falta-de-manganeso', 'Falta-de-potasio', 'Falta-nitrogeno'],\n",
    "        # Se mapea para que los índices de D2 sigan a los de D1 (0 a 4)\n",
    "        \"mapping\": {0: 5, 1: 6, 2: 7, 3: 8, 4: 9, 5: 10, 6: 11, 7: 12}\n",
    "    },\n",
    "    \"D8\": {\n",
    "        \"path\": os.path.join(base_path, \"D8 - Coffee Leaves Detection.v1i.yolov11\"),\n",
    "        \"names\": ['Miner', 'Rust'],\n",
    "        # Se mapea: \"Miner\" se asume igual que \"miner\" (índice 2) y \"Rust\" a \"rust\" (índice 4)\n",
    "        \"mapping\": {0: 2, 1: 4}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lista combinada de nombres (la unión de las clases, en el orden deseado)\n",
    "combined_names = ['cerscospora', 'healthy', 'miner', 'phoma', 'rust',\n",
    "                  'Falta-de-boro', 'Falta-de-calcio', 'Falta-de-fosforo', 'Falta-de-hierro', \n",
    "                  'Falta-de-magnesio', 'Falta-de-manganeso', 'Falta-de-potasio', 'Falta-nitrogeno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR ESTRUCTURA DE CARPETAS COMBINADA\n",
    "\n",
    "# Se creará una única carpeta \"combinado\" con subcarpetas \"train\", \"valid\" y \"test\"\n",
    "combined_path = os.path.join(base_path, \"combinado\")\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    os.makedirs(os.path.join(combined_path, split, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(combined_path, split, \"labels\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/ALEX/OneDrive/Cursos/Maestría en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/Datasets\\\\combinado'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path = os.path.normpath(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset D1 ...\n",
      "Procesando dataset D2 ...\n",
      "Procesando dataset D8 ...\n"
     ]
    }
   ],
   "source": [
    "# COPIAR Y REMAPEAR LOS ARCHIVOS DE CADA DATASET\n",
    "\n",
    "def copy_and_remap_dataset(dataset_name, dataset_info):\n",
    "    \"\"\"\n",
    "    Copia imágenes y etiquetas del dataset original a la estructura combinada.\n",
    "    Renombra los archivos para evitar conflictos y remapea los índices de clase.\n",
    "    \"\"\"\n",
    "    src_path = dataset_info[\"path\"]\n",
    "    mapping = dataset_info[\"mapping\"]\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        src_images = os.path.join(src_path, split, \"images\")\n",
    "        src_labels = os.path.join(src_path, split, \"labels\")\n",
    "        dst_images = os.path.join(combined_path, split, \"images\")\n",
    "        dst_labels = os.path.join(combined_path, split, \"labels\")\n",
    "        \n",
    "        # Copiar imágenes (se renombra con el prefijo del dataset)\n",
    "        if os.path.exists(src_images):\n",
    "            for fname in os.listdir(src_images):\n",
    "                src_file = os.path.join(src_images, fname)\n",
    "                new_fname = f\"{dataset_name}_{fname}\"\n",
    "                dst_file = os.path.join(dst_images, new_fname)\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "        else:\n",
    "            print(f\"Advertencia: No existe la carpeta {src_images}\")\n",
    "        \n",
    "        # Copiar etiquetas y remapear índices\n",
    "        if os.path.exists(src_labels):\n",
    "            for fname in os.listdir(src_labels):\n",
    "                src_file = os.path.join(src_labels, fname)\n",
    "                new_fname = f\"{dataset_name}_{fname}\"\n",
    "                dst_file = os.path.join(dst_labels, new_fname)\n",
    "                with open(src_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                new_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        try:\n",
    "                            cls_old = int(parts[0])\n",
    "                            cls_new = mapping.get(cls_old, cls_old)\n",
    "                            new_line = str(cls_new) + \" \" + \" \".join(parts[1:]) + \"\\n\"\n",
    "                            new_lines.append(new_line)\n",
    "                        except Exception as e:\n",
    "                            new_lines.append(line)\n",
    "                    else:\n",
    "                        new_lines.append(line)\n",
    "                with open(dst_file, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "        else:\n",
    "            print(f\"Advertencia: No existe la carpeta {src_labels}\")\n",
    "\n",
    "# Procesar cada dataset\n",
    "for ds in datasets:\n",
    "    print(f\"Procesando dataset {ds} ...\")\n",
    "    copy_and_remap_dataset(ds, datasets[ds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo inicial de imágenes por clase en TRAIN (combinado):\n",
      "  Clase 0 (cerscospora): 224 imágenes\n",
      "  Clase 1 (healthy): 187 imágenes\n",
      "  Clase 2 (miner): 347 imágenes\n",
      "  Clase 3 (phoma): 169 imágenes\n",
      "  Clase 4 (rust): 342 imágenes\n",
      "  Clase 5 (Falta-de-boro): 68 imágenes\n",
      "  Clase 6 (Falta-de-calcio): 112 imágenes\n",
      "  Clase 7 (Falta-de-fosforo): 157 imágenes\n",
      "  Clase 8 (Falta-de-hierro): 45 imágenes\n",
      "  Clase 9 (Falta-de-magnesio): 52 imágenes\n",
      "  Clase 10 (Falta-de-manganeso): 57 imágenes\n",
      "  Clase 11 (Falta-de-potasio): 67 imágenes\n",
      "  Clase 12 (Falta-nitrogeno): 45 imágenes\n"
     ]
    }
   ],
   "source": [
    "# CONTAR IMÁGENES POR CLASE EN TRAIN (CONJUNTO COMBINADO)\n",
    "\n",
    "def count_images_per_class_combined(combined_path, combined_names):\n",
    "    counts = {i: 0 for i in range(len(combined_names))}\n",
    "    labels_dir = os.path.join(combined_path, \"train\", \"labels\")\n",
    "    for fname in os.listdir(labels_dir):\n",
    "        file_path = os.path.join(labels_dir, fname)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            file_classes = set()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if parts:\n",
    "                    try:\n",
    "                        cls = int(parts[0])\n",
    "                        file_classes.add(cls)\n",
    "                    except:\n",
    "                        continue\n",
    "            for cls in file_classes:\n",
    "                if cls in counts:\n",
    "                    counts[cls] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {file_path}: {e}\")\n",
    "    return counts\n",
    "\n",
    "initial_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "print(\"Conteo inicial de imágenes por clase en TRAIN (combinado):\")\n",
    "for cls, count in initial_counts.items():\n",
    "    print(f\"  Clase {cls} ({combined_names[cls]}): {count} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando el dataaumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objetivo de imágenes por clase (TRAIN): 347\n",
      "Ronda de augmentación 0: mínimo actual = 45\n",
      "Ronda de augmentación 1: mínimo actual = 90\n",
      "Ronda de augmentación 2: mínimo actual = 135\n",
      "Ronda de augmentación 3: mínimo actual = 180\n",
      "Ronda de augmentación 4: mínimo actual = 225\n",
      "Ronda de augmentación 5: mínimo actual = 270\n",
      "Ronda de augmentación 6: mínimo actual = 315\n",
      "Ronda de augmentación 7: mínimo actual = 347\n",
      "\n",
      "Conteo final de imágenes por clase en TRAIN (combinado):\n",
      "  Clase 0 (cerscospora): 448 imágenes\n",
      "  Clase 1 (healthy): 374 imágenes\n",
      "  Clase 2 (miner): 347 imágenes\n",
      "  Clase 3 (phoma): 507 imágenes\n",
      "  Clase 4 (rust): 684 imágenes\n",
      "  Clase 5 (Falta-de-boro): 408 imágenes\n",
      "  Clase 6 (Falta-de-calcio): 448 imágenes\n",
      "  Clase 7 (Falta-de-fosforo): 471 imágenes\n",
      "  Clase 8 (Falta-de-hierro): 360 imágenes\n",
      "  Clase 9 (Falta-de-magnesio): 364 imágenes\n",
      "  Clase 10 (Falta-de-manganeso): 399 imágenes\n",
      "  Clase 11 (Falta-de-potasio): 402 imágenes\n",
      "  Clase 12 (Falta-nitrogeno): 360 imágenes\n"
     ]
    }
   ],
   "source": [
    "# DATA AUGMENTATION PARA HOMOGENIZAR LAS CLASES EN TRAIN\n",
    "\n",
    "def find_image_file(image_dir, base_name):\n",
    "    \"\"\"\n",
    "    Busca el archivo de imagen en image_dir que coincida con base_name\n",
    "    comprobando las extensiones más comunes.\n",
    "    \"\"\"\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        candidate = os.path.join(image_dir, base_name + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def imread_with_pil(path):\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            return np.array(im.convert(\"RGB\"))\n",
    "    except Exception as e:\n",
    "        print(\"Error al leer con PIL:\", path, e)\n",
    "        return None\n",
    "\n",
    "def augment_image_label(image_path, label_path, out_image_path, out_label_path, angle, brightness_factor):\n",
    "    # Intentar leer con cv2\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        # Si falla, intentar con PIL\n",
    "        img = imread_with_pil(image_path)\n",
    "        if img is None:\n",
    "            print(\"Error al leer la imagen:\", image_path)\n",
    "            return\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Aplicar rotación según el ángulo especificado\n",
    "    if angle != 0:\n",
    "        if angle == 90:\n",
    "            img_aug = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif angle == 180:\n",
    "            img_aug = cv2.rotate(img, cv2.ROTATE_180)\n",
    "        elif angle == 270:\n",
    "            img_aug = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        else:\n",
    "            img_aug = img.copy()\n",
    "    else:\n",
    "        img_aug = img.copy()\n",
    "    \n",
    "    # Ajustar brillo\n",
    "    img_aug = np.clip(img_aug * brightness_factor, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Guardar imagen aumentada\n",
    "    cv2.imwrite(out_image_path, img_aug)\n",
    "    \n",
    "    # Procesar etiqueta: ajustar las coordenadas según la rotación\n",
    "    new_lines = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if parts and len(parts) == 5:\n",
    "                try:\n",
    "                    cls = parts[0]\n",
    "                    x = float(parts[1])\n",
    "                    y = float(parts[2])\n",
    "                    bw = float(parts[3])\n",
    "                    bh = float(parts[4])\n",
    "                    # Ajustar coordenadas para rotaciones en formato YOLO\n",
    "                    if angle == 90:\n",
    "                        new_x = y\n",
    "                        new_y = 1 - x\n",
    "                        new_bw = bh\n",
    "                        new_bh = bw\n",
    "                    elif angle == 180:\n",
    "                        new_x = 1 - x\n",
    "                        new_y = 1 - y\n",
    "                        new_bw = bw\n",
    "                        new_bh = bh\n",
    "                    elif angle == 270:\n",
    "                        new_x = 1 - y\n",
    "                        new_y = x\n",
    "                        new_bw = bh\n",
    "                        new_bh = bw\n",
    "                    else:\n",
    "                        new_x, new_y, new_bw, new_bh = x, y, bw, bh\n",
    "                    new_line = f\"{cls} {new_x:.6f} {new_y:.6f} {new_bw:.6f} {new_bh:.6f}\\n\"\n",
    "                    new_lines.append(new_line)\n",
    "                except Exception as e:\n",
    "                    new_lines.append(line)\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "        with open(out_label_path, 'w') as f:\n",
    "            f.writelines(new_lines)\n",
    "    else:\n",
    "        open(out_label_path, 'w').close()\n",
    "\n",
    "# Definir el objetivo: igualar al máximo número de imágenes por clase en TRAIN\n",
    "current_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "target_count = max(current_counts.values())\n",
    "print(f\"\\nObjetivo de imágenes por clase (TRAIN): {target_count}\")\n",
    "\n",
    "train_images_dir = os.path.join(combined_path, \"train\", \"images\")\n",
    "train_labels_dir = os.path.join(combined_path, \"train\", \"labels\")\n",
    "label_files = os.listdir(train_labels_dir)\n",
    "\n",
    "augmented_counter = 0\n",
    "augmentation_round = 0\n",
    "max_rounds = 8  # para evitar ciclos infinitos\n",
    "\n",
    "while True:\n",
    "    current_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "    min_count = min(current_counts.values())\n",
    "    print(f\"Ronda de augmentación {augmentation_round}: mínimo actual = {min_count}\")\n",
    "    if min_count >= target_count or augmentation_round >= max_rounds:\n",
    "        break\n",
    "    # Recorre cada archivo de etiqueta en TRAIN\n",
    "    for fname in label_files:\n",
    "        label_path = os.path.join(train_labels_dir, fname)\n",
    "        # Leer clases presentes en la imagen\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        if not lines:\n",
    "            continue\n",
    "        image_classes = set()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if parts:\n",
    "                try:\n",
    "                    cls = int(parts[0])\n",
    "                    image_classes.add(cls)\n",
    "                except:\n",
    "                    continue\n",
    "        # Aquí se modifica la condición:\n",
    "        # Solo se aumenta si PARA TODAS las clases de la imagen el conteo actual es menor que target_count.\n",
    "        underrepresented = all(current_counts[cls] < target_count for cls in image_classes)\n",
    "        if underrepresented:\n",
    "            # Se supone que el nombre de la etiqueta y de la imagen son iguales (sin la extensión)\n",
    "            base_name = os.path.splitext(fname)[0]\n",
    "            # Buscar la imagen con extensión correcta\n",
    "            image_path = find_image_file(train_images_dir, base_name)\n",
    "            if image_path is None:\n",
    "                print(\"No se encontró la imagen para:\", base_name)\n",
    "                continue\n",
    "            # Nombres para los archivos aumentados\n",
    "            aug_image_name = base_name + f\"_aug{augmented_counter}\"\n",
    "            aug_label_name = base_name + f\"_aug{augmented_counter}.txt\"\n",
    "            # Mantener la misma extensión que la imagen encontrada\n",
    "            _, ext = os.path.splitext(image_path)\n",
    "            out_image_path = os.path.join(train_images_dir, aug_image_name + ext)\n",
    "            out_label_path = os.path.join(train_labels_dir, aug_label_name)\n",
    "            # Parámetros aleatorios de augmentación\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            brightness_factor = random.uniform(0.7, 1.3)\n",
    "            augment_image_label(image_path, label_path, out_image_path, out_label_path, angle, brightness_factor)\n",
    "            augmented_counter += 1\n",
    "    augmentation_round += 1\n",
    "\n",
    "final_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "print(\"\\nConteo final de imágenes por clase en TRAIN (combinado):\")\n",
    "for cls, count in final_counts.items():\n",
    "    print(f\"  Clase {cls} ({combined_names[cls]}): {count} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo final de imágenes por clase en TRAIN (combinado):\n",
      "  Clase 0 (cerscospora): 448 imágenes\n",
      "  Clase 1 (healthy): 374 imágenes\n",
      "  Clase 2 (miner): 347 imágenes\n",
      "  Clase 3 (phoma): 507 imágenes\n",
      "  Clase 4 (rust): 684 imágenes\n",
      "  Clase 5 (Falta-de-boro): 408 imágenes\n",
      "  Clase 6 (Falta-de-calcio): 448 imágenes\n",
      "  Clase 7 (Falta-de-fosforo): 471 imágenes\n",
      "  Clase 8 (Falta-de-hierro): 360 imágenes\n",
      "  Clase 9 (Falta-de-magnesio): 364 imágenes\n",
      "  Clase 10 (Falta-de-manganeso): 399 imágenes\n",
      "  Clase 11 (Falta-de-potasio): 402 imágenes\n",
      "  Clase 12 (Falta-nitrogeno): 360 imágenes\n"
     ]
    }
   ],
   "source": [
    "initial_counts = count_images_per_class_combined(combined_path, combined_names)\n",
    "print(\"Conteo final de imágenes por clase en TRAIN (combinado):\")\n",
    "for cls, count in initial_counts.items():\n",
    "    print(f\"  Clase {cls} ({combined_names[cls]}): {count} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data.yaml combinado generado en: C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\Datasets\\combinado\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "# GENERAR data.yaml COMBINADO\n",
    "\n",
    "data_yaml = {\n",
    "    \"train\": os.path.join(combined_path, \"train\", \"images\").replace(\"\\\\\", \"/\"),\n",
    "    \"val\": os.path.join(combined_path, \"valid\", \"images\").replace(\"\\\\\", \"/\"),\n",
    "    \"test\": os.path.join(combined_path, \"test\", \"images\").replace(\"\\\\\", \"/\"),\n",
    "    \"nc\": len(combined_names),\n",
    "    \"names\": combined_names\n",
    "}\n",
    "yaml_path = DATA_DIR / 'processed' / 'combinado' / 'data.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "print(\"\\ndata.yaml combinado generado en:\", yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import onnxruntime as ort\n",
    "import shutil\n",
    "import subprocess\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación y extracción de métricas\n",
    "def extract_metrics(model):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo usando model.val() y extrae las métricas de validación.\n",
    "    \n",
    "    Si el objeto retornado no es un diccionario, se intenta extraer atributos públicos.\n",
    "    Si no se obtiene una métrica clave (por ejemplo, \"metrics/precision(B)\"), se intenta leer\n",
    "    la última fila del CSV de resultados ubicado en 'runs/detect/train/results.csv'.\n",
    "    \n",
    "    Retorna:\n",
    "      - metrics_dict (dict): Diccionario con las métricas.\n",
    "    \"\"\"\n",
    "    metrics_obj = model.val()\n",
    "    if isinstance(metrics_obj, dict):\n",
    "        metrics_dict = metrics_obj\n",
    "    else:\n",
    "        try:\n",
    "            metrics_dict = metrics_obj.results_dict()\n",
    "        except Exception as e:\n",
    "            print(\"No se pudo usar results_dict(), se intentará extrayendo atributos.\", e)\n",
    "            try:\n",
    "                metrics_dict = {attr: getattr(metrics_obj, attr) \n",
    "                                for attr in dir(metrics_obj)\n",
    "                                if not attr.startswith('_') and not callable(getattr(metrics_obj, attr))}\n",
    "            except Exception as e:\n",
    "                print(\"Error extrayendo atributos del objeto de validación:\", e)\n",
    "                metrics_dict = {}\n",
    "    \n",
    "    # Verificar si se obtuvo la métrica clave; de lo contrario, leer el CSV\n",
    "    if not metrics_dict or \"metrics/precision(B)\" not in metrics_dict:\n",
    "        try:\n",
    "            csv_path = r\"runs\\detect\\train\\results.csv\"\n",
    "            if os.path.exists(csv_path):\n",
    "                results_df = pd.read_csv(csv_path)\n",
    "                # Se toma la última fila, que corresponde a las métricas finales\n",
    "                final_metrics = results_df.iloc[-1].to_dict()\n",
    "                metrics_dict = final_metrics\n",
    "        except Exception as e:\n",
    "            print(\"Error leyendo el CSV de resultados:\", e)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el modelo a ONNX y copiarlo con el nombre deseado\n",
    "def export_and_copy_onnx(model, model_name, imgsz, opset=12):\n",
    "    \"\"\"\n",
    "    Exporta el modelo a ONNX usando model.export(). Por defecto, Ultralytics guarda el ONNX\n",
    "    como 'best.onnx' en 'runs/detect/train/weights'. Esta función espera a que dicho archivo exista,\n",
    "    lo copia a la carpeta 'exports' con el nombre f\"{model_name}.onnx\" y retorna la ruta de destino.\n",
    "    \n",
    "    Se fuerza el parámetro opset para evitar problemas de compatibilidad (por ejemplo, Constant version 19).\n",
    "    \n",
    "    Parámetros:\n",
    "        - model: Instancia del modelo YOLO.\n",
    "        - model_name (str): Nombre identificativo del modelo.\n",
    "        - imgsz (int): Tamaño de la imagen.\n",
    "        - opset (int): Versión del opset para exportar (por ejemplo, 12).\n",
    "    \n",
    "    Retorna:\n",
    "        - onnx_dest_path (str): Ruta al archivo ONNX copiado en la carpeta 'exports'.\n",
    "    \"\"\"\n",
    "    onnx_dest_path = os.path.join(\"exports\", f\"{model_name}.onnx\")\n",
    "    os.makedirs(\"exports\", exist_ok=True)\n",
    "    \n",
    "    # Exporta el modelo con el opset especificado\n",
    "    model.export(format=\"onnx\", imgsz=imgsz, exist_ok=True, opset=opset)\n",
    "    \n",
    "    default_onnx_path = MODEL_DIR / 'fine_tuned' / 'YOLO11n.onnx'\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    # Esperar a que default_onnx_path se cree\n",
    "    while not os.path.exists(default_onnx_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if os.path.exists(default_onnx_path):\n",
    "        shutil.copy2(default_onnx_path, onnx_dest_path)\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el archivo ONNX por defecto en {default_onnx_path}\")\n",
    "    \n",
    "    # Esperar a que el archivo copiado exista en exports\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(onnx_dest_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if not os.path.exists(onnx_dest_path):\n",
    "        print(f\"Error: No se encontró el archivo ONNX en {onnx_dest_path}\")\n",
    "    \n",
    "    return onnx_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el modelo a TensorRT y copiarlo con el nombre deseado\n",
    "def export_and_copy_trt(model, model_name, imgsz, opset=12):\n",
    "    \"\"\"\n",
    "    Exporta el modelo a TensorRT usando model.export(format=\"trt\"). Por defecto, \n",
    "    Ultralytics guarda el engine en 'runs/detect/train/weights' como 'best.trt'. \n",
    "    Esta función espera a que dicho archivo exista, lo copia a la carpeta 'exports' con el nombre\n",
    "    f\"{model_name}.trt\" y retorna la ruta de destino.\n",
    "    \n",
    "    Se fuerza el parámetro opset para la conversión. Si tensorrt no está instalado, se captura la excepción.\n",
    "    \n",
    "    Parámetros:\n",
    "        - model: Instancia del modelo YOLO.\n",
    "        - model_name (str): Nombre identificativo del modelo.\n",
    "        - imgsz (int): Tamaño de la imagen.\n",
    "        - opset (int): Versión del opset para exportar.\n",
    "    \n",
    "    Retorna:\n",
    "        - trt_dest_path (str) o None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.export(format=\"trt\", imgsz=imgsz, exist_ok=True, opset=opset)\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(\"TensorRT no está instalado. Se retornará None para la velocidad TRT.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"Error exportando a TensorRT:\", e)\n",
    "        return None\n",
    "\n",
    "    trt_dest_path = os.path.join(\"exports\", f\"{model_name}.trt\")\n",
    "    os.makedirs(\"exports\", exist_ok=True)\n",
    "    \n",
    "    default_trt_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.trt\")\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(default_trt_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if os.path.exists(default_trt_path):\n",
    "        shutil.copy2(default_trt_path, trt_dest_path)\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el archivo TRT por defecto en {default_trt_path}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(trt_dest_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if not os.path.exists(trt_dest_path):\n",
    "        print(f\"Error: No se encontró el archivo TRT en {trt_dest_path}\")\n",
    "        return None\n",
    "    return trt_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para exportar y copiar el checkpoint en formato PT\n",
    "def export_and_copy_pt(model, model_name):\n",
    "    \"\"\"\n",
    "    Copia el checkpoint del modelo en formato .pt. Por defecto, Ultralytics guarda el checkpoint\n",
    "    como 'best.pt' en 'runs/detect/train/weights'. Esta función espera a que dicho archivo exista,\n",
    "    lo copia a la carpeta 'exports' con el nombre f\"{model_name}.pt\" y retorna la ruta de destino.\n",
    "    \n",
    "    Retorna:\n",
    "      - pt_dest_path (str): Ruta al archivo .pt copiado en la carpeta 'exports'.\n",
    "    \"\"\"\n",
    "    pt_dest_path = os.path.join(\"exports\", f\"{model_name}.pt\")\n",
    "    os.makedirs(\"exports\", exist_ok=True)\n",
    "    \n",
    "    default_pt_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\")\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    # Esperar a que se cree el checkpoint por defecto\n",
    "    while not os.path.exists(default_pt_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if os.path.exists(default_pt_path):\n",
    "        shutil.copy2(default_pt_path, pt_dest_path)\n",
    "    else:\n",
    "        print(f\"Error: No se encontró el archivo PT por defecto en {default_pt_path}\")\n",
    "    \n",
    "    # Esperar a que el archivo copiado exista en exports\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(pt_dest_path) and (time.time() - start_time) < timeout:\n",
    "        time.sleep(1)\n",
    "    if not os.path.exists(pt_dest_path):\n",
    "        print(f\"Error: No se encontró el archivo PT en {pt_dest_path}\")\n",
    "    \n",
    "    return pt_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para medir la velocidad de inferencia en CPU usando ONNX\n",
    "def measure_onnx_inference_speed(onnx_path, imgsz, n_iter=20):\n",
    "    \"\"\"\n",
    "    Mide la velocidad de inferencia en CPU usando onnxruntime.\n",
    "    \n",
    "    Parámetros:\n",
    "      - onnx_path (str): Ruta al archivo ONNX.\n",
    "      - imgsz (int): Tamaño de la imagen (ancho y alto).\n",
    "      - n_iter (int): Número de iteraciones para promediar.\n",
    "    \n",
    "    Retorna:\n",
    "      - Tiempo promedio en milisegundos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ort_session = ort.InferenceSession(onnx_path)\n",
    "        dummy_input = np.random.rand(1, 3, imgsz, imgsz).astype(np.float32)\n",
    "        # Warm-up\n",
    "        for _ in range(5):\n",
    "            _ = ort_session.run(None, {\"images\": dummy_input})\n",
    "        start = time.time()\n",
    "        for _ in range(n_iter):\n",
    "            _ = ort_session.run(None, {\"images\": dummy_input})\n",
    "        end = time.time()\n",
    "        avg_time = (end - start) / n_iter * 1000  # ms\n",
    "        return avg_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error midiendo ONNX runtime: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir la velocidad de inferencia en T4 TensorRT usando trtexec\n",
    "def measure_trt_inference_speed(trt_engine_path, n_iter=20):\n",
    "    \"\"\"\n",
    "    Mide la velocidad de inferencia en una GPU T4 utilizando la herramienta 'trtexec'.\n",
    "    Se asume que 'trtexec' está instalado y en el PATH.\n",
    "    \n",
    "    Parámetros:\n",
    "      - trt_engine_path (str): Ruta al engine de TensorRT (archivo .trt).\n",
    "      - n_iter (int): Número de iteraciones para promediar.\n",
    "    \n",
    "    Retorna:\n",
    "      - Tiempo promedio en milisegundos (float) o None en caso de error.\n",
    "    \"\"\"\n",
    "    if trt_engine_path is None:\n",
    "        return None\n",
    "    try:\n",
    "        cmd = [\"trtexec\", f\"--loadEngine={trt_engine_path}\", f\"--iterations={n_iter}\"]\n",
    "        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "        stdout, stderr = proc.communicate(timeout=120)\n",
    "        avg_time = None\n",
    "        for line in stdout.splitlines():\n",
    "            if \"Average latency\" in line:\n",
    "                parts = line.split(\":\")\n",
    "                if len(parts) > 1:\n",
    "                    avg_time = float(parts[1].strip().split()[0])\n",
    "                    break\n",
    "        if avg_time is None:\n",
    "            print(\"No se pudo extraer el tiempo promedio de la salida de trtexec.\")\n",
    "        return avg_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error midiendo TensorRT runtime: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calular los flops\n",
    "def compute_flops(model, imgsz):\n",
    "    # get_model_complexity_info devuelve un string y el número total de FLOPs\n",
    "    # Asegúrate de que model.model es el objeto PyTorch con el que se pueda contar FLOPs.\n",
    "    macs, params = get_model_complexity_info(model.model, (3, imgsz, imgsz), as_strings=False, print_per_layer_stat=False)\n",
    "    flops = 2 * macs  # MACs a FLOPs (multiplicamos por 2)\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular parámetros y combinar todos los resultados\n",
    "def compute_model_params(model):\n",
    "    \"\"\"\n",
    "    Calcula la cantidad de parámetros del modelo en millones.\n",
    "    \n",
    "    Retorna:\n",
    "      - params (float): Número de parámetros en millones.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.model.parameters()) / 1e6\n",
    "\n",
    "def combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops):\n",
    "    \"\"\"\n",
    "    Combina el nombre del modelo, el tamaño de la imagen, las métricas, la velocidad de inferencia,\n",
    "    la cantidad de parámetros y los FLOPs en un único diccionario.\n",
    "    \n",
    "    Retorna:\n",
    "      - results (dict): Diccionario con toda la información.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"Modelo\": model_name,\n",
    "        \"Tamaño (píxeles)\": imgsz,\n",
    "    }\n",
    "    results.update(metrics_dict)\n",
    "    results.update({\n",
    "        \"Velocidad CPU ONNX (ms)\": avg_time_cpu,\n",
    "        \"Velocidad T4 TensorRT (ms)\": avg_time_trt,\n",
    "        \"Parámetros (M)\": params,\n",
    "        \"FLOPs (B)\": flops\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Fine Tuning y evaluación de modelos\n",
    "\n",
    "# Lista de modelos a fine-tunear. Debes tener disponibles los pesos preentrenados.\n",
    "models_info = [\n",
    "    {\"name\": \"YOLO11n\", \"weights\": MODEL_DIR / \"base\" / \"yolo11n.pt\"},\n",
    "    {\"name\": \"YOLO12n\", \"weights\": MODEL_DIR / \"base\" / \"yolo12n.pt\"},\n",
    "    {\"name\": \"YOLO11s\", \"weights\": MODEL_DIR / \"base\" / \"yolo11s.pt\"},\n",
    "    {\"name\": \"YOLO12s\", \"weights\": MODEL_DIR / \"base\" / \"yolo12s.pt\"}\n",
    "]\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "epochs = 50\n",
    "imgsz = 640\n",
    "batch = 16\n",
    "patience = 5\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Ruta al archivo data.yaml del dataset combinado\n",
    "yaml_path = DATA_DIR / 'processed' / 'combinado' / 'data.yaml'\n",
    "\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.84 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml, epochs=50, time=None, patience=5, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    433207  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,592,375 parameters, 2,592,359 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.32G     0.8398      3.475      1.371         13        640: 100%|██████████| 118/118 [00:12<00:00,  9.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.246      0.589      0.279      0.227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.38G     0.8346      2.189      1.347         11        640: 100%|██████████| 118/118 [00:10<00:00, 10.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.417      0.606      0.512      0.384\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50       2.4G     0.8006      1.823      1.309         15        640: 100%|██████████| 118/118 [00:10<00:00, 11.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.42      0.655      0.526      0.399\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.38G     0.7638      1.627      1.273         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.503      0.644      0.603      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      2.38G     0.7609      1.498      1.271         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.514      0.743      0.652      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.37G     0.7229      1.404      1.256         17        640: 100%|██████████| 118/118 [00:10<00:00, 11.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.671      0.704      0.683      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      2.38G     0.7207       1.32      1.238         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.64      0.733      0.726      0.628\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      2.38G     0.7109      1.222       1.24         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.633      0.719      0.734       0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.38G     0.6752       1.19      1.211         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.711      0.723      0.768      0.674\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.37G     0.6739      1.099      1.207         11        640: 100%|██████████| 118/118 [00:10<00:00, 11.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.668      0.792      0.788      0.692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.38G     0.6835      1.116      1.213         16        640: 100%|██████████| 118/118 [00:10<00:00, 11.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.639       0.72      0.751      0.631\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      2.37G     0.6543      1.068      1.199          8        640: 100%|██████████| 118/118 [00:10<00:00, 11.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.683      0.745      0.767      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      2.38G      0.665      1.037      1.211         13        640: 100%|██████████| 118/118 [00:10<00:00, 11.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.676      0.749      0.717      0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      2.37G     0.6551     0.9921      1.189         12        640: 100%|██████████| 118/118 [00:10<00:00, 11.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.668      0.793      0.812      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      2.37G     0.6346     0.9672      1.188         11        640: 100%|██████████| 118/118 [00:10<00:00, 11.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.751      0.807      0.833      0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      2.36G     0.6392     0.9555      1.183          9        640: 100%|██████████| 118/118 [00:10<00:00, 11.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.743      0.795      0.832      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      2.37G      0.654     0.9412      1.185         11        640: 100%|██████████| 118/118 [00:10<00:00, 11.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.755       0.81      0.836      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      2.38G     0.6159     0.9038      1.166         14        640: 100%|██████████| 118/118 [00:10<00:00, 11.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.749       0.85      0.843       0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      2.38G     0.6093     0.8908      1.162         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.762      0.784      0.827      0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      2.38G     0.6051     0.8733      1.157         16        640: 100%|██████████| 118/118 [00:10<00:00, 11.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.77       0.83      0.853      0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      2.37G     0.5926     0.8362      1.155         12        640: 100%|██████████| 118/118 [00:10<00:00, 11.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.77      0.849      0.852      0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      2.37G     0.6129     0.8505      1.158         19        640: 100%|██████████| 118/118 [00:10<00:00, 11.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.799      0.833       0.88      0.798\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      2.38G     0.5966     0.8187      1.154         13        640: 100%|██████████| 118/118 [00:10<00:00, 11.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.821      0.788      0.857      0.773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      2.38G     0.6053     0.8117      1.154          8        640: 100%|██████████| 118/118 [00:10<00:00, 11.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.792      0.848      0.878      0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      2.37G     0.6182     0.8116      1.158         14        640: 100%|██████████| 118/118 [00:10<00:00, 11.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.83      0.822      0.884      0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      2.37G     0.5901     0.7897      1.143         13        640: 100%|██████████| 118/118 [00:10<00:00, 11.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.819      0.861      0.886      0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      2.38G     0.5603     0.7646      1.129         15        640: 100%|██████████| 118/118 [00:10<00:00, 11.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472        0.8      0.877      0.888      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      2.38G     0.5744     0.7645      1.134          9        640: 100%|██████████| 118/118 [00:10<00:00, 11.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.825      0.868      0.904       0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      2.38G     0.5638     0.7595      1.128          9        640: 100%|██████████| 118/118 [00:10<00:00, 11.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.825      0.883        0.9      0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      2.38G     0.5707     0.7223      1.135         12        640: 100%|██████████| 118/118 [00:10<00:00, 11.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.847      0.856      0.898      0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      2.38G     0.5587     0.7237      1.124          9        640: 100%|██████████| 118/118 [00:10<00:00, 11.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.852      0.879      0.917      0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      2.38G     0.5563     0.7322      1.117         12        640: 100%|██████████| 118/118 [00:10<00:00, 11.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.844      0.834      0.902      0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      2.38G     0.5487       0.69      1.117         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.849      0.883      0.917      0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      2.37G     0.5608      0.705      1.125         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.889      0.883      0.928      0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      2.38G     0.5434     0.6745      1.117          9        640: 100%|██████████| 118/118 [00:10<00:00, 11.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.887      0.871      0.923      0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      2.38G       0.55     0.6654      1.112         13        640: 100%|██████████| 118/118 [00:10<00:00, 11.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.856      0.904       0.93       0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      2.37G     0.5455     0.6821      1.113         12        640: 100%|██████████| 118/118 [00:10<00:00, 11.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.867      0.912      0.941      0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      2.38G     0.5508      0.652      1.111         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.855      0.924      0.934      0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      2.37G      0.543     0.6443      1.111         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.895      0.892      0.946      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      2.37G     0.5419     0.6452      1.105         12        640: 100%|██████████| 118/118 [00:10<00:00, 11.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.891      0.918      0.946      0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      2.37G     0.5698     0.5892      1.131         10        640: 100%|██████████| 118/118 [00:10<00:00, 11.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.914      0.914      0.957      0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      2.37G     0.5596      0.562      1.134          4        640: 100%|██████████| 118/118 [00:09<00:00, 11.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.896      0.924      0.956       0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      2.36G     0.5507     0.5406      1.127          4        640: 100%|██████████| 118/118 [00:09<00:00, 11.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.894      0.924      0.955      0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      2.36G     0.5439     0.5306      1.114          8        640: 100%|██████████| 118/118 [00:09<00:00, 11.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.892      0.904      0.945      0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      2.36G     0.5427      0.517      1.108          8        640: 100%|██████████| 118/118 [00:10<00:00, 11.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.902      0.933       0.96      0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      2.36G     0.5337     0.5067      1.112          5        640: 100%|██████████| 118/118 [00:09<00:00, 11.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.907      0.932      0.964      0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      2.36G     0.5312     0.4947      1.102          7        640: 100%|██████████| 118/118 [00:09<00:00, 11.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.905      0.939      0.964      0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      2.36G     0.5209      0.477      1.101          8        640: 100%|██████████| 118/118 [00:09<00:00, 11.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.908      0.946      0.967      0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      2.36G     0.5165     0.4716      1.093          6        640: 100%|██████████| 118/118 [00:10<00:00, 11.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.923      0.937      0.969       0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      2.36G     0.5157     0.4596      1.095          4        640: 100%|██████████| 118/118 [00:09<00:00, 11.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:06<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.924      0.936       0.97      0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.245 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,584,687 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.923      0.936       0.97      0.911\n",
      "           cerscospora        224        224      0.996      0.999      0.994      0.965\n",
      "               healthy        187        190      0.963      0.963      0.981       0.83\n",
      "                 miner        347        709       0.96      0.983       0.99      0.793\n",
      "                 phoma        169        169      0.997          1      0.995      0.974\n",
      "                  rust        342        577      0.892       0.79      0.932      0.629\n",
      "         Falta-de-boro         68         68       0.97      0.985       0.99       0.99\n",
      "       Falta-de-calcio        112        112      0.926      0.991      0.991      0.987\n",
      "      Falta-de-fosforo        157        157      0.968      0.981      0.994      0.987\n",
      "       Falta-de-hierro         45         45      0.855      0.933      0.958      0.938\n",
      "     Falta-de-magnesio         52         52      0.761      0.904      0.915      0.915\n",
      "    Falta-de-manganeso         57         57      0.847      0.807       0.92      0.915\n",
      "      Falta-de-potasio         67         67      0.894      0.877      0.967      0.967\n",
      "       Falta-nitrogeno         45         45      0.977      0.949      0.984      0.959\n",
      "Speed: 0.1ms preprocess, 0.7ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Procesar el primer modelo: YOLO11n\n",
    "info = models_info[0]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo (fine tuning)\n",
    "train_results = model.train(project=OUTPUT_DIR / \"runs\", \n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,584,687 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 118/118 [00:07<00:00, 14.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.924      0.936       0.97      0.912\n",
      "           cerscospora        224        224      0.996      0.999      0.994      0.967\n",
      "               healthy        187        190      0.963      0.963      0.981       0.83\n",
      "                 miner        347        709      0.959      0.982       0.99      0.794\n",
      "                 phoma        169        169      0.997          1      0.995      0.974\n",
      "                  rust        342        577      0.896       0.79      0.932       0.63\n",
      "         Falta-de-boro         68         68       0.97      0.985       0.99       0.99\n",
      "       Falta-de-calcio        112        112      0.926      0.991      0.991      0.987\n",
      "      Falta-de-fosforo        157        157      0.968      0.981      0.994      0.987\n",
      "       Falta-de-hierro         45         45      0.855      0.933      0.958      0.938\n",
      "     Falta-de-magnesio         52         52      0.763      0.904      0.916      0.914\n",
      "    Falta-de-manganeso         57         57      0.847      0.807       0.92      0.915\n",
      "      Falta-de-potasio         67         67      0.893      0.876      0.966      0.966\n",
      "       Falta-nitrogeno         45         45      0.977       0.95      0.984      0.959\n",
      "Speed: 0.2ms preprocess, 1.1ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "No se pudo usar results_dict(), se intentará extrayendo atributos. 'dict' object is not callable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 50.0,\n",
       " 'time': 883.183,\n",
       " 'train/box_loss': 0.51573,\n",
       " 'train/cls_loss': 0.45963,\n",
       " 'train/dfl_loss': 1.09464,\n",
       " 'metrics/precision(B)': 0.9237,\n",
       " 'metrics/recall(B)': 0.93555,\n",
       " 'metrics/mAP50(B)': 0.96995,\n",
       " 'metrics/mAP50-95(B)': 0.91132,\n",
       " 'val/box_loss': 0.48951,\n",
       " 'val/cls_loss': 0.41305,\n",
       " 'val/dfl_loss': 0.89411,\n",
       " 'lr/pg0': 1.75224e-05,\n",
       " 'lr/pg1': 1.75224e-05,\n",
       " 'lr/pg2': 1.75224e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CPU (AMD Ryzen 7 7700X 8-Core Processor)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.2s, saved as 'runs\\detect\\train\\weights\\best.onnx' (10.1 MB)\n",
      "\n",
      "Export complete (1.3s)\n",
      "Results saved to \u001b[1mC:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.9s, saved as 'runs\\detect\\train\\weights\\best.onnx' (10.1 MB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure  0.9s: No module named 'tensorrt'\n",
      "TensorRT no está instalado. Se retornará None para la velocidad TRT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Modelo': 'YOLO11n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 883.183,\n",
       "  'train/box_loss': 0.51573,\n",
       "  'train/cls_loss': 0.45963,\n",
       "  'train/dfl_loss': 1.09464,\n",
       "  'metrics/precision(B)': 0.9237,\n",
       "  'metrics/recall(B)': 0.93555,\n",
       "  'metrics/mAP50(B)': 0.96995,\n",
       "  'metrics/mAP50-95(B)': 0.91132,\n",
       "  'val/box_loss': 0.48951,\n",
       "  'val/cls_loss': 0.41305,\n",
       "  'val/dfl_loss': 0.89411,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 22.33635187149048,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.584687,\n",
       "  'FLOPs (B)': 6393562400}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo11n = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo11n)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO12n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.84 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo12n.pt, data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml, epochs=50, time=None, patience=5, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    433207  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
      "YOLOv12n summary: 272 layers, 2,570,583 parameters, 2,570,567 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      3.29G     0.8255      3.341      1.368         13        640: 100%|██████████| 118/118 [00:16<00:00,  7.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.521      0.398       0.36      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      3.42G     0.8672      2.122      1.364         11        640: 100%|██████████| 118/118 [00:14<00:00,  7.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.395      0.571      0.493      0.361\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      3.42G      0.826      1.784      1.317         15        640: 100%|██████████| 118/118 [00:14<00:00,  8.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.439      0.681      0.534      0.414\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      3.41G     0.7918        1.6      1.282         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.459      0.685      0.619      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50       3.4G     0.7789       1.49       1.28         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.507      0.718      0.665      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50       3.4G     0.7457      1.385      1.258         17        640: 100%|██████████| 118/118 [00:14<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.587      0.703       0.67      0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      3.42G     0.7414      1.302      1.254         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.612      0.739      0.697      0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      3.41G     0.7335      1.216      1.248         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.785      0.618      0.717      0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      3.41G      0.697      1.202      1.223         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.648      0.713       0.72      0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50       3.4G     0.7095      1.103      1.229         11        640: 100%|██████████| 118/118 [00:14<00:00,  8.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.646      0.772      0.724      0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      3.41G     0.6933      1.099      1.219         16        640: 100%|██████████| 118/118 [00:14<00:00,  8.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.638      0.761      0.735      0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50       3.4G     0.6667      1.065      1.206          8        640: 100%|██████████| 118/118 [00:14<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.658      0.779      0.748      0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      3.41G      0.684       1.04       1.22         13        640: 100%|██████████| 118/118 [00:14<00:00,  8.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.603      0.811      0.754      0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50       3.4G     0.6717     0.9812      1.201         12        640: 100%|██████████| 118/118 [00:14<00:00,  8.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.667      0.814      0.776      0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      3.39G     0.6465     0.9585      1.196         11        640: 100%|██████████| 118/118 [00:14<00:00,  8.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.668      0.801      0.783      0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      3.39G     0.6555     0.9502      1.192          9        640: 100%|██████████| 118/118 [00:14<00:00,  8.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.624      0.825       0.77      0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50       3.4G     0.6675     0.9145      1.201         11        640: 100%|██████████| 118/118 [00:14<00:00,  8.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.678      0.833      0.784      0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50       3.4G     0.6357     0.8989      1.178         14        640: 100%|██████████| 118/118 [00:14<00:00,  8.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.724      0.799        0.8      0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      3.42G     0.6264     0.8903      1.171         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.734      0.815      0.826      0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      3.41G     0.6184     0.8593      1.173         16        640: 100%|██████████| 118/118 [00:14<00:00,  8.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.707      0.794      0.787      0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50       3.4G     0.6072     0.8331      1.162         12        640: 100%|██████████| 118/118 [00:14<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.752      0.828      0.841      0.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      3.39G     0.6202     0.8393      1.169         19        640: 100%|██████████| 118/118 [00:14<00:00,  8.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.769       0.82      0.822      0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      3.41G     0.6069     0.8046      1.167         13        640: 100%|██████████| 118/118 [00:14<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.723      0.834      0.811      0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      3.41G     0.6239     0.8049       1.17          8        640: 100%|██████████| 118/118 [00:14<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.741       0.84      0.842      0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50       3.4G     0.6441     0.7953      1.176         14        640: 100%|██████████| 118/118 [00:14<00:00,  8.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.777      0.852      0.846      0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      3.39G     0.6028     0.7872      1.155         13        640: 100%|██████████| 118/118 [00:14<00:00,  8.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.766      0.859      0.858      0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      3.41G     0.5808     0.7651      1.147         15        640: 100%|██████████| 118/118 [00:14<00:00,  8.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.791       0.82       0.86      0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      3.41G     0.5916     0.7402      1.147          9        640: 100%|██████████| 118/118 [00:14<00:00,  8.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.82      0.854      0.889      0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50       3.4G     0.5683     0.7414      1.138          9        640: 100%|██████████| 118/118 [00:14<00:00,  8.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.803      0.811      0.862      0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50       3.4G     0.5881     0.7224      1.147         12        640: 100%|██████████| 118/118 [00:14<00:00,  8.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.811      0.847      0.879      0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50       3.4G     0.5656     0.7075      1.134          9        640: 100%|██████████| 118/118 [00:14<00:00,  8.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.817      0.867      0.887      0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      3.41G     0.5737     0.7042      1.131         12        640: 100%|██████████| 118/118 [00:13<00:00,  8.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.843      0.869      0.904      0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      3.41G     0.5642     0.6713      1.131         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.828       0.88      0.903      0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50       3.4G      0.562     0.6889      1.132         10        640: 100%|██████████| 118/118 [00:13<00:00,  8.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.858      0.889      0.907       0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      3.41G     0.5502      0.651      1.125          9        640: 100%|██████████| 118/118 [00:14<00:00,  8.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.868      0.866      0.904      0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      3.41G     0.5639     0.6576      1.126         13        640: 100%|██████████| 118/118 [00:13<00:00,  8.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.877      0.872      0.917      0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50       3.4G      0.555     0.6523      1.128         12        640: 100%|██████████| 118/118 [00:13<00:00,  8.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.831      0.876       0.91      0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      3.41G     0.5532     0.6327      1.118         10        640: 100%|██████████| 118/118 [00:13<00:00,  8.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.876      0.877      0.916      0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      3.39G     0.5479     0.6205      1.121         10        640: 100%|██████████| 118/118 [00:13<00:00,  8.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.871      0.888      0.925      0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50       3.4G     0.5431     0.6167      1.115         12        640: 100%|██████████| 118/118 [00:13<00:00,  8.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.857      0.901      0.935       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50       3.4G     0.5723     0.5564      1.141         10        640: 100%|██████████| 118/118 [00:14<00:00,  8.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.868      0.904      0.925      0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      3.39G     0.5634     0.5324       1.14          4        640: 100%|██████████| 118/118 [00:13<00:00,  8.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.877      0.905       0.94      0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      3.39G     0.5557      0.512      1.128          4        640: 100%|██████████| 118/118 [00:13<00:00,  8.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.878      0.916      0.944      0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      3.39G     0.5541     0.5017      1.124          8        640: 100%|██████████| 118/118 [00:13<00:00,  8.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.915      0.905      0.952      0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      3.39G     0.5439      0.486      1.119          8        640: 100%|██████████| 118/118 [00:13<00:00,  8.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.911      0.908      0.948      0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      3.39G     0.5308     0.4773      1.117          5        640: 100%|██████████| 118/118 [00:13<00:00,  8.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.914      0.901      0.953      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      3.39G     0.5334     0.4719      1.108          7        640: 100%|██████████| 118/118 [00:13<00:00,  8.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472        0.9      0.921       0.95      0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      3.39G     0.5207     0.4596      1.106          8        640: 100%|██████████| 118/118 [00:13<00:00,  8.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.91      0.921      0.958        0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      3.39G     0.5162     0.4495      1.104          6        640: 100%|██████████| 118/118 [00:13<00:00,  8.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.908      0.953      0.965      0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      3.39G     0.5113     0.4363      1.094          4        640: 100%|██████████| 118/118 [00:13<00:00,  8.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.912      0.956      0.968      0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.313 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 5.6MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 5.6MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,559,263 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.912      0.956      0.968      0.911\n",
      "           cerscospora        224        224      0.992          1      0.994      0.973\n",
      "               healthy        187        190      0.947      0.963       0.98      0.831\n",
      "                 miner        347        709      0.952      0.989      0.993      0.801\n",
      "                 phoma        169        169      0.996          1      0.995      0.976\n",
      "                  rust        342        577      0.877      0.862      0.937      0.634\n",
      "         Falta-de-boro         68         68      0.983          1      0.994      0.994\n",
      "       Falta-de-calcio        112        112       0.93      0.991      0.986      0.984\n",
      "      Falta-de-fosforo        157        157      0.977      0.987      0.994      0.989\n",
      "       Falta-de-hierro         45         45      0.862      0.956      0.961      0.946\n",
      "     Falta-de-magnesio         52         52      0.742      0.923      0.898      0.898\n",
      "    Falta-de-manganeso         57         57      0.842       0.86      0.915      0.909\n",
      "      Falta-de-potasio         67         67      0.897      0.905      0.973      0.973\n",
      "       Falta-nitrogeno         45         45      0.865      0.997       0.96      0.938\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Procesar el primer modelo: YOLO12n\n",
    "info = models_info[1]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo:\n",
    "train_results = model.train(project=OUTPUT_DIR / \"runs\", \n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,559,263 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 118/118 [00:08<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.912      0.956      0.968      0.911\n",
      "           cerscospora        224        224      0.992          1      0.994      0.973\n",
      "               healthy        187        190      0.947      0.963       0.98      0.831\n",
      "                 miner        347        709      0.952      0.989      0.993        0.8\n",
      "                 phoma        169        169      0.996          1      0.995      0.976\n",
      "                  rust        342        577      0.873      0.862      0.937      0.635\n",
      "         Falta-de-boro         68         68      0.983          1      0.994      0.994\n",
      "       Falta-de-calcio        112        112       0.93      0.991      0.986      0.984\n",
      "      Falta-de-fosforo        157        157      0.977      0.987      0.994      0.989\n",
      "       Falta-de-hierro         45         45      0.862      0.956      0.961      0.943\n",
      "     Falta-de-magnesio         52         52      0.741      0.923      0.899      0.899\n",
      "    Falta-de-manganeso         57         57      0.844       0.86      0.915      0.909\n",
      "      Falta-de-potasio         67         67      0.896      0.905      0.973      0.973\n",
      "       Falta-nitrogeno         45         45      0.865      0.998       0.96      0.938\n",
      "Speed: 0.2ms preprocess, 1.6ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "No se pudo usar results_dict(), se intentará extrayendo atributos. 'dict' object is not callable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 50.0,\n",
       " 'time': 1127.04,\n",
       " 'train/box_loss': 0.51129,\n",
       " 'train/cls_loss': 0.43628,\n",
       " 'train/dfl_loss': 1.0939,\n",
       " 'metrics/precision(B)': 0.91198,\n",
       " 'metrics/recall(B)': 0.95641,\n",
       " 'metrics/mAP50(B)': 0.96774,\n",
       " 'metrics/mAP50-95(B)': 0.91137,\n",
       " 'val/box_loss': 0.47812,\n",
       " 'val/cls_loss': 0.38077,\n",
       " 'val/dfl_loss': 0.88336,\n",
       " 'lr/pg0': 1.75224e-05,\n",
       " 'lr/pg1': 1.75224e-05,\n",
       " 'lr/pg2': 1.75224e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CPU (AMD Ryzen 7 7700X 8-Core Processor)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.6s, saved as 'runs\\detect\\train\\weights\\best.onnx' (10.1 MB)\n",
      "\n",
      "Export complete (1.7s)\n",
      "Results saved to \u001b[1mC:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.4s, saved as 'runs\\detect\\train\\weights\\best.onnx' (10.1 MB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure  1.4s: No module named 'tensorrt'\n",
      "TensorRT no está instalado. Se retornará None para la velocidad TRT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Modelo': 'YOLO11n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 883.183,\n",
       "  'train/box_loss': 0.51573,\n",
       "  'train/cls_loss': 0.45963,\n",
       "  'train/dfl_loss': 1.09464,\n",
       "  'metrics/precision(B)': 0.9237,\n",
       "  'metrics/recall(B)': 0.93555,\n",
       "  'metrics/mAP50(B)': 0.96995,\n",
       "  'metrics/mAP50-95(B)': 0.91132,\n",
       "  'val/box_loss': 0.48951,\n",
       "  'val/cls_loss': 0.41305,\n",
       "  'val/dfl_loss': 0.89411,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 22.33635187149048,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.584687,\n",
       "  'FLOPs (B)': 6393562400},\n",
       " {'Modelo': 'YOLO12n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 1127.04,\n",
       "  'train/box_loss': 0.51129,\n",
       "  'train/cls_loss': 0.43628,\n",
       "  'train/dfl_loss': 1.0939,\n",
       "  'metrics/precision(B)': 0.91198,\n",
       "  'metrics/recall(B)': 0.95641,\n",
       "  'metrics/mAP50(B)': 0.96774,\n",
       "  'metrics/mAP50-95(B)': 0.91137,\n",
       "  'val/box_loss': 0.47812,\n",
       "  'val/cls_loss': 0.38077,\n",
       "  'val/dfl_loss': 0.88336,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 25.344526767730713,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.559263,\n",
       "  'FLOPs (B)': 6409127200}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo12n = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo12n)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.84 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11s.pt, data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml, epochs=50, time=None, patience=5, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    824439  ultralytics.nn.modules.head.Detect           [13, [128, 256, 512]]         \n",
      "YOLO11s summary: 181 layers, 9,432,823 parameters, 9,432,807 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      4.02G     0.8155      2.975      1.365         13        640: 100%|██████████| 118/118 [00:15<00:00,  7.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.531      0.574      0.512      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      4.12G      0.784      1.482      1.295         11        640: 100%|██████████| 118/118 [00:13<00:00,  8.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.48      0.589      0.539      0.409\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      4.12G     0.7909      1.346      1.277         15        640: 100%|██████████| 118/118 [00:13<00:00,  9.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.322      0.634      0.453      0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      4.09G     0.7624      1.232      1.265         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.604      0.573      0.632       0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50       4.1G     0.7698        1.2      1.259         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.726      0.486      0.585      0.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      4.08G     0.7246      1.135      1.247         17        640: 100%|██████████| 118/118 [00:12<00:00,  9.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.533       0.64      0.636      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      4.11G       0.73      1.103      1.241         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.601      0.736      0.724      0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      4.09G     0.7266      1.024      1.241         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.619      0.761      0.723      0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50       4.1G     0.6886      1.027      1.214         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.608      0.774      0.756       0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      4.08G     0.6896     0.9474      1.212         11        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.644      0.774      0.763      0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      4.11G     0.6935     0.9683      1.209         16        640: 100%|██████████| 118/118 [00:12<00:00,  9.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.669      0.728      0.773      0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      4.08G     0.6622     0.9214      1.192          8        640: 100%|██████████| 118/118 [00:12<00:00,  9.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.731      0.786      0.805      0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      4.11G     0.6749     0.9292      1.209         13        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.691       0.83      0.813       0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      4.08G     0.6649      0.888      1.186         12        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472        0.7      0.787      0.792        0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      4.09G     0.6407     0.8621      1.181         11        640: 100%|██████████| 118/118 [00:12<00:00,  9.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.677      0.797      0.789      0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      4.08G     0.6442     0.8773      1.181          9        640: 100%|██████████| 118/118 [00:12<00:00,  9.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.707      0.758      0.826       0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      4.09G     0.6677     0.8448      1.189         11        640: 100%|██████████| 118/118 [00:13<00:00,  9.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.742        0.8      0.822      0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      4.09G     0.6317     0.8357       1.17         14        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.75      0.798      0.836      0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      4.11G     0.6235     0.8142      1.164         10        640: 100%|██████████| 118/118 [00:13<00:00,  9.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.775      0.812      0.832      0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      4.09G     0.6073     0.7828      1.154         16        640: 100%|██████████| 118/118 [00:12<00:00,  9.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.726        0.8      0.833      0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50       4.1G     0.6068     0.7684       1.15         12        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.735       0.78      0.811      0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      4.08G     0.6163     0.7836      1.156         19        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.778      0.829      0.871      0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50       4.1G     0.6114     0.7484      1.158         13        640: 100%|██████████| 118/118 [00:13<00:00,  9.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472        0.8      0.845      0.879      0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      4.09G     0.6157     0.7438      1.155          8        640: 100%|██████████| 118/118 [00:13<00:00,  9.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.831      0.837      0.883      0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      4.09G     0.6299     0.7451      1.161         14        640: 100%|██████████| 118/118 [00:12<00:00,  9.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.799      0.808      0.846      0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      4.09G     0.5939     0.7363      1.144         13        640: 100%|██████████| 118/118 [00:12<00:00,  9.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.743      0.851      0.869      0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      4.11G     0.5686     0.7012      1.132         15        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.814      0.855      0.891      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      4.09G     0.5899      0.695       1.14          9        640: 100%|██████████| 118/118 [00:12<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.857      0.854       0.91      0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      4.11G     0.5674     0.6857      1.128          9        640: 100%|██████████| 118/118 [00:12<00:00,  9.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.851      0.828      0.894      0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      4.09G     0.5772      0.679      1.134         12        640: 100%|██████████| 118/118 [00:12<00:00,  9.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.826      0.871      0.897      0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50       4.1G     0.5554     0.6534      1.121          9        640: 100%|██████████| 118/118 [00:13<00:00,  9.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.857      0.883       0.92      0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      4.09G     0.5636     0.6722      1.116         12        640: 100%|██████████| 118/118 [00:12<00:00,  9.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.838      0.881      0.915      0.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50       4.1G     0.5563     0.6299       1.12         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.847      0.865      0.908       0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      4.08G     0.5616     0.6488      1.124         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.892      0.862      0.925      0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50       4.1G     0.5493     0.6103      1.116          9        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.853      0.855      0.901       0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      4.09G     0.5621     0.6167      1.114         13        640: 100%|██████████| 118/118 [00:12<00:00,  9.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.879      0.865      0.921      0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      4.09G     0.5486      0.617      1.112         12        640: 100%|██████████| 118/118 [00:12<00:00,  9.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.889      0.873      0.932      0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      4.09G     0.5503      0.588      1.106         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.894      0.879      0.928      0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      4.09G     0.5377     0.5828      1.107         10        640: 100%|██████████| 118/118 [00:12<00:00,  9.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.883      0.889      0.941      0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      4.08G     0.5397     0.5787      1.103         12        640: 100%|██████████| 118/118 [00:12<00:00,  9.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.898      0.892       0.94      0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      4.09G     0.5678     0.5034      1.122         10        640: 100%|██████████| 118/118 [00:13<00:00,  9.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.888      0.893      0.942      0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      4.08G     0.5533     0.4885      1.122          4        640: 100%|██████████| 118/118 [00:12<00:00,  9.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.875      0.898      0.943      0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      4.09G     0.5506     0.4667      1.118          4        640: 100%|██████████| 118/118 [00:12<00:00,  9.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.88      0.897      0.948      0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      4.07G     0.5402     0.4545      1.107          8        640: 100%|██████████| 118/118 [00:12<00:00,  9.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.899      0.895      0.945      0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      4.09G     0.5435     0.4478      1.097          8        640: 100%|██████████| 118/118 [00:12<00:00,  9.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.902      0.909      0.954      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      4.07G     0.5226     0.4321        1.1          5        640: 100%|██████████| 118/118 [00:12<00:00,  9.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.907      0.918      0.957      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      4.09G     0.5319     0.4274      1.098          7        640: 100%|██████████| 118/118 [00:12<00:00,  9.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.905      0.899      0.951      0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      4.08G      0.515     0.4059       1.09          8        640: 100%|██████████| 118/118 [00:12<00:00,  9.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.918      0.915       0.96      0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      4.09G     0.5123     0.4039      1.084          6        640: 100%|██████████| 118/118 [00:12<00:00,  9.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.924      0.917      0.963      0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      4.07G     0.5034     0.3928      1.078          4        640: 100%|██████████| 118/118 [00:12<00:00,  9.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:07<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.916       0.93      0.965      0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.307 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 19.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 19.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,417,831 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.916       0.93      0.965      0.908\n",
      "           cerscospora        224        224      0.995          1      0.994      0.978\n",
      "               healthy        187        190      0.936      0.937      0.974      0.783\n",
      "                 miner        347        709      0.967      0.987      0.993      0.813\n",
      "                 phoma        169        169      0.997          1      0.995      0.974\n",
      "                  rust        342        577      0.916       0.87      0.953      0.673\n",
      "         Falta-de-boro         68         68      0.985      0.981      0.983      0.983\n",
      "       Falta-de-calcio        112        112       0.94       0.98      0.987      0.983\n",
      "      Falta-de-fosforo        157        157      0.967      0.987      0.994      0.988\n",
      "       Falta-de-hierro         45         45      0.785      0.933      0.941      0.926\n",
      "     Falta-de-magnesio         52         52      0.708      0.788      0.867      0.867\n",
      "    Falta-de-manganeso         57         57      0.833      0.875      0.919      0.916\n",
      "      Falta-de-potasio         67         67      0.962      0.821      0.977      0.977\n",
      "       Falta-nitrogeno         45         45      0.912      0.924       0.97       0.95\n",
      "Speed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Procesar el primer modelo: YOLO11s\n",
    "info = models_info[2]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo:\n",
    "train_results = model.train(project=OUTPUT_DIR / \"runs\", \n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,417,831 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 118/118 [00:09<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.916       0.93      0.965      0.908\n",
      "           cerscospora        224        224      0.995          1      0.993      0.977\n",
      "               healthy        187        190      0.936      0.937      0.974      0.782\n",
      "                 miner        347        709      0.967      0.987      0.993      0.813\n",
      "                 phoma        169        169      0.997          1      0.995      0.975\n",
      "                  rust        342        577      0.916       0.87      0.953      0.673\n",
      "         Falta-de-boro         68         68      0.985      0.981      0.983      0.983\n",
      "       Falta-de-calcio        112        112       0.94       0.98      0.987      0.983\n",
      "      Falta-de-fosforo        157        157      0.967      0.987      0.994      0.988\n",
      "       Falta-de-hierro         45         45      0.785      0.933      0.941      0.926\n",
      "     Falta-de-magnesio         52         52      0.708      0.788      0.866      0.866\n",
      "    Falta-de-manganeso         57         57      0.833      0.875      0.919      0.916\n",
      "      Falta-de-potasio         67         67      0.962      0.821      0.977      0.977\n",
      "       Falta-nitrogeno         45         45      0.912      0.925       0.97       0.95\n",
      "Speed: 0.1ms preprocess, 2.2ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "No se pudo usar results_dict(), se intentará extrayendo atributos. 'dict' object is not callable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 50.0,\n",
       " 'time': 1103.55,\n",
       " 'train/box_loss': 0.50343,\n",
       " 'train/cls_loss': 0.39276,\n",
       " 'train/dfl_loss': 1.07776,\n",
       " 'metrics/precision(B)': 0.91603,\n",
       " 'metrics/recall(B)': 0.92962,\n",
       " 'metrics/mAP50(B)': 0.96505,\n",
       " 'metrics/mAP50-95(B)': 0.9083,\n",
       " 'val/box_loss': 0.46924,\n",
       " 'val/cls_loss': 0.34404,\n",
       " 'val/dfl_loss': 0.87531,\n",
       " 'lr/pg0': 1.75224e-05,\n",
       " 'lr/pg1': 1.75224e-05,\n",
       " 'lr/pg2': 1.75224e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CPU (AMD Ryzen 7 7700X 8-Core Processor)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (18.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.1s, saved as 'runs\\detect\\train\\weights\\best.onnx' (36.2 MB)\n",
      "\n",
      "Export complete (1.3s)\n",
      "Results saved to \u001b[1mC:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (18.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.4s, saved as 'runs\\detect\\train\\weights\\best.onnx' (36.2 MB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure  1.4s: No module named 'tensorrt'\n",
      "TensorRT no está instalado. Se retornará None para la velocidad TRT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Modelo': 'YOLO11n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 883.183,\n",
       "  'train/box_loss': 0.51573,\n",
       "  'train/cls_loss': 0.45963,\n",
       "  'train/dfl_loss': 1.09464,\n",
       "  'metrics/precision(B)': 0.9237,\n",
       "  'metrics/recall(B)': 0.93555,\n",
       "  'metrics/mAP50(B)': 0.96995,\n",
       "  'metrics/mAP50-95(B)': 0.91132,\n",
       "  'val/box_loss': 0.48951,\n",
       "  'val/cls_loss': 0.41305,\n",
       "  'val/dfl_loss': 0.89411,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 22.33635187149048,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.584687,\n",
       "  'FLOPs (B)': 6393562400},\n",
       " {'Modelo': 'YOLO12n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 1127.04,\n",
       "  'train/box_loss': 0.51129,\n",
       "  'train/cls_loss': 0.43628,\n",
       "  'train/dfl_loss': 1.0939,\n",
       "  'metrics/precision(B)': 0.91198,\n",
       "  'metrics/recall(B)': 0.95641,\n",
       "  'metrics/mAP50(B)': 0.96774,\n",
       "  'metrics/mAP50-95(B)': 0.91137,\n",
       "  'val/box_loss': 0.47812,\n",
       "  'val/cls_loss': 0.38077,\n",
       "  'val/dfl_loss': 0.88336,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 25.344526767730713,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.559263,\n",
       "  'FLOPs (B)': 6409127200},\n",
       " {'Modelo': 'YOLO11s',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 1103.55,\n",
       "  'train/box_loss': 0.50343,\n",
       "  'train/cls_loss': 0.39276,\n",
       "  'train/dfl_loss': 1.07776,\n",
       "  'metrics/precision(B)': 0.91603,\n",
       "  'metrics/recall(B)': 0.92962,\n",
       "  'metrics/mAP50(B)': 0.96505,\n",
       "  'metrics/mAP50-95(B)': 0.9083,\n",
       "  'val/box_loss': 0.46924,\n",
       "  'val/cls_loss': 0.34404,\n",
       "  'val/dfl_loss': 0.87531,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 52.62967348098755,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 9.417831,\n",
       "  'FLOPs (B)': 21457421600}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo11s = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo11s)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de YOLO12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.84 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo12s.pt, data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml, epochs=50, time=None, patience=5, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
      " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 21        [14, 17, 20]  1    824439  ultralytics.nn.modules.head.Detect           [13, [128, 256, 512]]         \n",
      "YOLOv12s summary: 272 layers, 9,258,167 parameters, 9,258,151 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      6.01G     0.8418      2.519      1.389         13        640: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.533      0.579      0.522      0.408\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      6.08G     0.8516      1.532      1.365         11        640: 100%|██████████| 118/118 [00:19<00:00,  6.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.408      0.724      0.541      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      6.07G     0.8619      1.434      1.352         15        640: 100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.375      0.679      0.493      0.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      6.05G     0.8159      1.312      1.318         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.433      0.702      0.585      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      6.06G     0.8141      1.253      1.318         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.544       0.68      0.606      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      6.04G     0.7684      1.206      1.292         17        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.775      0.511      0.626      0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      6.06G     0.7704      1.157      1.287         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.534       0.74      0.681      0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      6.05G     0.7523      1.105       1.28         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.55      0.706      0.687      0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      6.06G     0.7146      1.075      1.247         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.617      0.779       0.75      0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      6.05G     0.7316      1.006      1.262         11        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.693      0.721       0.75      0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      6.07G     0.7181      1.007      1.246         16        640: 100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.612      0.778       0.75       0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      6.05G     0.6863      0.981      1.229          8        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.647      0.785      0.756      0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      6.06G      0.693     0.9581      1.235         13        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.673      0.792      0.745      0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      6.05G     0.6852      0.923       1.22         12        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:08<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.658      0.806      0.791      0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      6.06G     0.6612     0.9068      1.214         11        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.661      0.785      0.746      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      6.04G      0.668     0.9073      1.213          9        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.679      0.795      0.819      0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      6.06G      0.674     0.8507      1.217         11        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.726       0.83      0.825      0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      6.06G     0.6441     0.8494      1.196         14        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.703        0.8      0.793      0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      6.06G     0.6362     0.8295      1.193         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.729      0.803       0.83      0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      6.05G     0.6248     0.8005      1.186         16        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.713      0.817      0.828      0.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      6.06G     0.6241     0.7935      1.188         12        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.794      0.814      0.853      0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      6.05G     0.6277     0.8161      1.183         19        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.787      0.813      0.865      0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      6.06G     0.6214     0.7803      1.181         13        640: 100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.736      0.861      0.833      0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      6.05G     0.6274     0.7554      1.185          8        640: 100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.746      0.817      0.865      0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      6.06G      0.647     0.7648      1.193         14        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.774      0.841      0.855      0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      6.05G     0.6061     0.7342       1.17         13        640: 100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.792      0.837      0.876      0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      6.06G     0.5878     0.7257      1.158         15        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.785      0.858      0.876      0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      6.05G     0.6022     0.7044      1.167          9        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.831      0.852      0.896      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      6.07G     0.5808     0.7121      1.157          9        640: 100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.789      0.866       0.88      0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      6.05G     0.5959     0.6921      1.163         12        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.762      0.861      0.879      0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      6.06G     0.5751     0.6701      1.152          9        640: 100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.864      0.843      0.908      0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      6.05G     0.5799     0.6826      1.145         12        640: 100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.846      0.861      0.906       0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      6.06G     0.5658     0.6394      1.146         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.827      0.861        0.9      0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      6.05G     0.5713     0.6645      1.149         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.859       0.88       0.92      0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      6.06G     0.5569     0.6326      1.138          9        640: 100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.877      0.869      0.921      0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      6.05G     0.5764     0.6305      1.146         13        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.839       0.88      0.914      0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      6.06G     0.5608     0.6186       1.14         12        640: 100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.885      0.869      0.924      0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      6.05G     0.5628     0.5971      1.137         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472       0.87      0.891      0.927      0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      6.06G      0.558     0.5937      1.137         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.883      0.871      0.932      0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      6.05G     0.5556      0.581      1.133         12        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.873      0.886      0.931      0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      6.06G     0.5804     0.5095      1.158         10        640: 100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.882      0.914      0.938      0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      6.05G     0.5697     0.4928      1.159          4        640: 100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.902      0.893      0.948      0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      6.05G     0.5627     0.4849      1.157          4        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.874      0.897      0.936      0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      6.04G     0.5577     0.4578      1.149          8        640: 100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.879      0.904      0.946      0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      6.05G      0.557     0.4455      1.142          8        640: 100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.901      0.903      0.941       0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      6.04G     0.5431     0.4425      1.143          5        640: 100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.902      0.917      0.956      0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      6.05G     0.5411     0.4292      1.127          7        640: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.905      0.925       0.96      0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      6.04G     0.5265     0.4072      1.123          8        640: 100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.917      0.934      0.963      0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      6.05G     0.5315     0.4059      1.129          6        640: 100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.939      0.917      0.966      0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      6.04G     0.5161     0.3878      1.112          4        640: 100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.937      0.923      0.968       0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.401 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 19.0MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 19.0MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLOv12s summary (fused): 159 layers, 9,235,911 parameters, 0 gradients, 21.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 59/59 [00:09<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.938      0.923      0.968       0.91\n",
      "           cerscospora        224        224      0.996      0.991      0.994      0.976\n",
      "               healthy        187        190      0.972      0.924      0.983      0.809\n",
      "                 miner        347        709      0.964      0.976      0.993      0.799\n",
      "                 phoma        169        169      0.997          1      0.995      0.973\n",
      "                  rust        342        577      0.937      0.787      0.945      0.647\n",
      "         Falta-de-boro         68         68      0.966      0.985      0.987      0.987\n",
      "       Falta-de-calcio        112        112      0.947      0.964      0.984      0.979\n",
      "      Falta-de-fosforo        157        157      0.994      0.981      0.994      0.989\n",
      "       Falta-de-hierro         45         45      0.807      0.933      0.953      0.935\n",
      "     Falta-de-magnesio         52         52       0.82      0.808      0.893      0.893\n",
      "    Falta-de-manganeso         57         57      0.887      0.826      0.915      0.915\n",
      "      Falta-de-potasio         67         67      0.967      0.863      0.973      0.973\n",
      "       Falta-nitrogeno         45         45      0.935      0.959      0.973      0.949\n",
      "Speed: 0.1ms preprocess, 2.0ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Procesar el primer modelo: YOLO12s\n",
    "info = models_info[3]\n",
    "model_name = info[\"name\"]\n",
    "weights_path = info[\"weights\"]\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# 1. Entrenamiento del modelo:\n",
    "train_results = model.train(project=OUTPUT_DIR / \"runs\", \n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        exist_ok=True,\n",
    "        pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "YOLOv12s summary (fused): 159 layers, 9,235,911 parameters, 0 gradients, 21.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\labels.cache... 1876 images, 4 backgrounds, 0 corrupt: 100%|██████████| 1876/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\combinado\\train\\images\\D2_Fe-21-_jpg.rf.edc47339d1daf7530340e96080dc2f82.jpg: 1 duplicate labels removed\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 603, len(boxes) = 2472. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 118/118 [00:11<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1876       2472      0.937      0.923      0.968       0.91\n",
      "           cerscospora        224        224      0.996      0.991      0.994      0.976\n",
      "               healthy        187        190      0.972      0.924      0.983      0.808\n",
      "                 miner        347        709      0.964      0.976      0.993      0.799\n",
      "                 phoma        169        169      0.997          1      0.995      0.974\n",
      "                  rust        342        577      0.934      0.786      0.944      0.646\n",
      "         Falta-de-boro         68         68      0.966      0.985      0.987      0.987\n",
      "       Falta-de-calcio        112        112      0.946      0.964      0.984      0.979\n",
      "      Falta-de-fosforo        157        157      0.994      0.981      0.994      0.989\n",
      "       Falta-de-hierro         45         45      0.807      0.933      0.953      0.935\n",
      "     Falta-de-magnesio         52         52      0.819      0.808      0.893      0.893\n",
      "    Falta-de-manganeso         57         57      0.887      0.827      0.915      0.915\n",
      "      Falta-de-potasio         67         67      0.967      0.862      0.974      0.974\n",
      "       Falta-nitrogeno         45         45      0.935       0.96      0.973      0.949\n",
      "Speed: 0.2ms preprocess, 3.3ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "No se pudo usar results_dict(), se intentará extrayendo atributos. 'dict' object is not callable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 50.0,\n",
       " 'time': 1441.98,\n",
       " 'train/box_loss': 0.51612,\n",
       " 'train/cls_loss': 0.38781,\n",
       " 'train/dfl_loss': 1.11219,\n",
       " 'metrics/precision(B)': 0.93749,\n",
       " 'metrics/recall(B)': 0.92305,\n",
       " 'metrics/mAP50(B)': 0.9678,\n",
       " 'metrics/mAP50-95(B)': 0.9096,\n",
       " 'val/box_loss': 0.4873,\n",
       " 'val/cls_loss': 0.35821,\n",
       " 'val/dfl_loss': 0.90756,\n",
       " 'lr/pg0': 1.75224e-05,\n",
       " 'lr/pg1': 1.75224e-05,\n",
       " 'lr/pg2': 1.75224e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Evaluación y extracción de métricas\n",
    "metrics_dict = extract_metrics(model)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CPU (AMD Ryzen 7 7700X 8-Core Processor)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (18.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.9s, saved as 'runs\\detect\\train\\weights\\best.onnx' (35.5 MB)\n",
      "\n",
      "Export complete (2.2s)\n",
      "Results saved to \u001b[1mC:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 data=C:/Users/ALEX/OneDrive/Cursos/Maestra en Big Data y Data Science/Cursos-VIU/Oblgatorios/TFM/detection-diseases-coffee/combinado/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.82  Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (18.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.6s, saved as 'runs\\detect\\train\\weights\\best.onnx' (35.5 MB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure  1.6s: No module named 'tensorrt'\n",
      "TensorRT no está instalado. Se retornará None para la velocidad TRT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Modelo': 'YOLO11n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 883.183,\n",
       "  'train/box_loss': 0.51573,\n",
       "  'train/cls_loss': 0.45963,\n",
       "  'train/dfl_loss': 1.09464,\n",
       "  'metrics/precision(B)': 0.9237,\n",
       "  'metrics/recall(B)': 0.93555,\n",
       "  'metrics/mAP50(B)': 0.96995,\n",
       "  'metrics/mAP50-95(B)': 0.91132,\n",
       "  'val/box_loss': 0.48951,\n",
       "  'val/cls_loss': 0.41305,\n",
       "  'val/dfl_loss': 0.89411,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 22.33635187149048,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.584687,\n",
       "  'FLOPs (B)': 6393562400},\n",
       " {'Modelo': 'YOLO12n',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 1127.04,\n",
       "  'train/box_loss': 0.51129,\n",
       "  'train/cls_loss': 0.43628,\n",
       "  'train/dfl_loss': 1.0939,\n",
       "  'metrics/precision(B)': 0.91198,\n",
       "  'metrics/recall(B)': 0.95641,\n",
       "  'metrics/mAP50(B)': 0.96774,\n",
       "  'metrics/mAP50-95(B)': 0.91137,\n",
       "  'val/box_loss': 0.47812,\n",
       "  'val/cls_loss': 0.38077,\n",
       "  'val/dfl_loss': 0.88336,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 25.344526767730713,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 2.559263,\n",
       "  'FLOPs (B)': 6409127200},\n",
       " {'Modelo': 'YOLO11s',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 1103.55,\n",
       "  'train/box_loss': 0.50343,\n",
       "  'train/cls_loss': 0.39276,\n",
       "  'train/dfl_loss': 1.07776,\n",
       "  'metrics/precision(B)': 0.91603,\n",
       "  'metrics/recall(B)': 0.92962,\n",
       "  'metrics/mAP50(B)': 0.96505,\n",
       "  'metrics/mAP50-95(B)': 0.9083,\n",
       "  'val/box_loss': 0.46924,\n",
       "  'val/cls_loss': 0.34404,\n",
       "  'val/dfl_loss': 0.87531,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 52.62967348098755,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 9.417831,\n",
       "  'FLOPs (B)': 21457421600},\n",
       " {'Modelo': 'YOLO12s',\n",
       "  'Tamaño (píxeles)': 640,\n",
       "  'epoch': 50.0,\n",
       "  'time': 1441.98,\n",
       "  'train/box_loss': 0.51612,\n",
       "  'train/cls_loss': 0.38781,\n",
       "  'train/dfl_loss': 1.11219,\n",
       "  'metrics/precision(B)': 0.93749,\n",
       "  'metrics/recall(B)': 0.92305,\n",
       "  'metrics/mAP50(B)': 0.9678,\n",
       "  'metrics/mAP50-95(B)': 0.9096,\n",
       "  'val/box_loss': 0.4873,\n",
       "  'val/cls_loss': 0.35821,\n",
       "  'val/dfl_loss': 0.90756,\n",
       "  'lr/pg0': 1.75224e-05,\n",
       "  'lr/pg1': 1.75224e-05,\n",
       "  'lr/pg2': 1.75224e-05,\n",
       "  'Velocidad CPU ONNX (ms)': 63.152515888214104,\n",
       "  'Velocidad T4 TensorRT (ms)': None,\n",
       "  'Parámetros (M)': 9.235911,\n",
       "  'FLOPs (B)': 21383693600}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guardar ruta de exortación en formato pt\n",
    "pt_export_path = export_and_copy_pt(model, model_name)\n",
    "\n",
    "# 3. Exportar a ONNX y copiarlo con el nombre deseado\n",
    "onnx_export_path = export_and_copy_onnx(model, model_name, imgsz)\n",
    "avg_time_cpu = measure_onnx_inference_speed(onnx_export_path, imgsz)\n",
    "\n",
    "# 4. Exportar a TensorRT y medir velocidad (si es posible)\n",
    "trt_engine_path = export_and_copy_trt(model, model_name, imgsz)\n",
    "avg_time_trt = measure_trt_inference_speed(trt_engine_path, n_iter=20)\n",
    "\n",
    "# 5. Calcular parámetros y definir placeholder para FLOPs\n",
    "params = compute_model_params(model)\n",
    "\n",
    "flops = compute_flops(model, imgsz)\n",
    "\n",
    "# 6. Combinar toda la información\n",
    "results_yolo12s = combine_results(model_name, imgsz, metrics_dict, avg_time_cpu, avg_time_trt, params, flops)\n",
    "\n",
    "results_list.append(results_yolo12s)\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisión de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación de modelos fine-tuned:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Tamaño (píxeles)</th>\n",
       "      <th>epoch</th>\n",
       "      <th>time</th>\n",
       "      <th>train/box_loss</th>\n",
       "      <th>train/cls_loss</th>\n",
       "      <th>train/dfl_loss</th>\n",
       "      <th>metrics/precision(B)</th>\n",
       "      <th>metrics/recall(B)</th>\n",
       "      <th>metrics/mAP50(B)</th>\n",
       "      <th>...</th>\n",
       "      <th>val/box_loss</th>\n",
       "      <th>val/cls_loss</th>\n",
       "      <th>val/dfl_loss</th>\n",
       "      <th>lr/pg0</th>\n",
       "      <th>lr/pg1</th>\n",
       "      <th>lr/pg2</th>\n",
       "      <th>Velocidad CPU ONNX (ms)</th>\n",
       "      <th>Velocidad T4 TensorRT (ms)</th>\n",
       "      <th>Parámetros (M)</th>\n",
       "      <th>FLOPs (B)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>640</td>\n",
       "      <td>50.0</td>\n",
       "      <td>883.183</td>\n",
       "      <td>0.51573</td>\n",
       "      <td>0.45963</td>\n",
       "      <td>1.09464</td>\n",
       "      <td>0.92370</td>\n",
       "      <td>0.93555</td>\n",
       "      <td>0.96995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.41305</td>\n",
       "      <td>0.89411</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>22.336352</td>\n",
       "      <td>None</td>\n",
       "      <td>2.584687</td>\n",
       "      <td>6393562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOLO12n</td>\n",
       "      <td>640</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1127.040</td>\n",
       "      <td>0.51129</td>\n",
       "      <td>0.43628</td>\n",
       "      <td>1.09390</td>\n",
       "      <td>0.91198</td>\n",
       "      <td>0.95641</td>\n",
       "      <td>0.96774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47812</td>\n",
       "      <td>0.38077</td>\n",
       "      <td>0.88336</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>25.344527</td>\n",
       "      <td>None</td>\n",
       "      <td>2.559263</td>\n",
       "      <td>6409127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOLO11s</td>\n",
       "      <td>640</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1103.550</td>\n",
       "      <td>0.50343</td>\n",
       "      <td>0.39276</td>\n",
       "      <td>1.07776</td>\n",
       "      <td>0.91603</td>\n",
       "      <td>0.92962</td>\n",
       "      <td>0.96505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46924</td>\n",
       "      <td>0.34404</td>\n",
       "      <td>0.87531</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>52.629673</td>\n",
       "      <td>None</td>\n",
       "      <td>9.417831</td>\n",
       "      <td>21457421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOLO12s</td>\n",
       "      <td>640</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1441.980</td>\n",
       "      <td>0.51612</td>\n",
       "      <td>0.38781</td>\n",
       "      <td>1.11219</td>\n",
       "      <td>0.93749</td>\n",
       "      <td>0.92305</td>\n",
       "      <td>0.96780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48730</td>\n",
       "      <td>0.35821</td>\n",
       "      <td>0.90756</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>63.152516</td>\n",
       "      <td>None</td>\n",
       "      <td>9.235911</td>\n",
       "      <td>21383693600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Modelo  Tamaño (píxeles)  epoch      time  train/box_loss  train/cls_loss  \\\n",
       "0  YOLO11n               640   50.0   883.183         0.51573         0.45963   \n",
       "1  YOLO12n               640   50.0  1127.040         0.51129         0.43628   \n",
       "2  YOLO11s               640   50.0  1103.550         0.50343         0.39276   \n",
       "3  YOLO12s               640   50.0  1441.980         0.51612         0.38781   \n",
       "\n",
       "   train/dfl_loss  metrics/precision(B)  metrics/recall(B)  metrics/mAP50(B)  \\\n",
       "0         1.09464               0.92370            0.93555           0.96995   \n",
       "1         1.09390               0.91198            0.95641           0.96774   \n",
       "2         1.07776               0.91603            0.92962           0.96505   \n",
       "3         1.11219               0.93749            0.92305           0.96780   \n",
       "\n",
       "   ...  val/box_loss  val/cls_loss  val/dfl_loss    lr/pg0    lr/pg1  \\\n",
       "0  ...       0.48951       0.41305       0.89411  0.000018  0.000018   \n",
       "1  ...       0.47812       0.38077       0.88336  0.000018  0.000018   \n",
       "2  ...       0.46924       0.34404       0.87531  0.000018  0.000018   \n",
       "3  ...       0.48730       0.35821       0.90756  0.000018  0.000018   \n",
       "\n",
       "     lr/pg2  Velocidad CPU ONNX (ms)  Velocidad T4 TensorRT (ms)  \\\n",
       "0  0.000018                22.336352                        None   \n",
       "1  0.000018                25.344527                        None   \n",
       "2  0.000018                52.629673                        None   \n",
       "3  0.000018                63.152516                        None   \n",
       "\n",
       "  Parámetros (M)    FLOPs (B)  \n",
       "0       2.584687   6393562400  \n",
       "1       2.559263   6409127200  \n",
       "2       9.417831  21457421600  \n",
       "3       9.235911  21383693600  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear y mostrar la tabla comparativa\n",
    "df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"\\nComparación de modelos fine-tuned:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la tabla en CSV\n",
    "df.to_csv(\"comparacion_modelos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_metrics(df,keywords):\n",
    "    \"\"\"\n",
    "    A partir de un DataFrame con columnas de métricas (por ejemplo, provenientes del CSV de resultados)\n",
    "    detecta cuáles son las métricas más relevantes para comparar modelos de detección y genera un texto\n",
    "    explicativo de la elección.\n",
    "    \n",
    "    Se consideran relevantes las columnas que contengan las palabras clave:\n",
    "      - precision\n",
    "      - recall\n",
    "      - mAP50-95\n",
    "      - mAP50\n",
    "    Adicionalmente, se sugieren opcionalmente columnas como 'epoch' y 'time' para analizar eficiencia.\n",
    "\n",
    "    Retorna:\n",
    "      - relevant_metrics: lista de nombres de columnas relevantes\n",
    "      - optional_metrics: lista de columnas opcionales (como 'epoch' o 'time')\n",
    "      - explanation: texto explicativo de la elección\n",
    "    \"\"\"\n",
    "    # Palabras clave para métricas principales (sin importar mayúsculas o minúsculas)\n",
    "    relevant_metrics = []\n",
    "    for col in df.columns:\n",
    "        for key in keywords:\n",
    "            if key.lower() in col.lower():\n",
    "                relevant_metrics.append(col)\n",
    "                break\n",
    "\n",
    "    # Métricas opcionales que se pueden usar para comparar eficiencia o duración\n",
    "    optional_metrics = []\n",
    "    for opt in ['time', 'epoch']:\n",
    "        if opt in df.columns:\n",
    "            optional_metrics.append(opt)\n",
    "    \n",
    "    explanation = \"Se han seleccionado las siguientes métricas principales para comparar el desempeño de detección:\\n\"\n",
    "    explanation += \", \".join(relevant_metrics) + \".\\n\\n\"\n",
    "    explanation += \"Estas métricas se consideran relevantes porque:\\n\"\n",
    "    explanation += \"- Las métricas con el prefijo 'metrics/' (por ejemplo, \" + \", \".join([m for m in relevant_metrics if 'metrics/' in m]) + \") \" \n",
    "    explanation += \"representan directamente la calidad de la detección (precisión, recall y mAP) en el conjunto de validación.\\n\"\n",
    "    explanation += \"- Los valores de 'mAP50' y 'mAP50-95' permiten evaluar el desempeño en diferentes umbrales de IoU, lo cual es crucial en tareas de detección.\\n\\n\"\n",
    "    explanation += \"Adicionalmente, se consideran opcionales las siguientes columnas para analizar la eficiencia y convergencia:\\n\"\n",
    "    explanation += \", \".join(optional_metrics) + \".\\n\"\n",
    "    explanation += \"Estas columnas indican el tiempo de entrenamiento y el número de epochs, lo que ayuda a comparar la velocidad de convergencia y el costo computacional, aunque no son indicadores directos de la calidad en inferencia.\\n\\n\"\n",
    "    explanation += \"Por otro lado, se omiten columnas con prefijos como 'train/' (pérdidas de entrenamiento) y 'lr/' (tasa de aprendizaje) ya que, aunque son útiles para monitorizar el proceso de optimización, no reflejan directamente el desempeño final en la tarea de detección.\"\n",
    "    \n",
    "    return relevant_metrics, optional_metrics, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han seleccionado las siguientes métricas principales para comparar el desempeño de detección:\n",
      "metrics/precision(B), metrics/recall(B), metrics/mAP50(B), metrics/mAP50-95(B).\n",
      "\n",
      "Estas métricas se consideran relevantes porque:\n",
      "- Las métricas con el prefijo 'metrics/' (por ejemplo, metrics/precision(B), metrics/recall(B), metrics/mAP50(B), metrics/mAP50-95(B)) representan directamente la calidad de la detección (precisión, recall y mAP) en el conjunto de validación.\n",
      "- Los valores de 'mAP50' y 'mAP50-95' permiten evaluar el desempeño en diferentes umbrales de IoU, lo cual es crucial en tareas de detección.\n",
      "\n",
      "Adicionalmente, se consideran opcionales las siguientes columnas para analizar la eficiencia y convergencia:\n",
      "time, epoch.\n",
      "Estas columnas indican el tiempo de entrenamiento y el número de epochs, lo que ayuda a comparar la velocidad de convergencia y el costo computacional, aunque no son indicadores directos de la calidad en inferencia.\n",
      "\n",
      "Por otro lado, se omiten columnas con prefijos como 'train/' (pérdidas de entrenamiento) y 'lr/' (tasa de aprendizaje) ya que, aunque son útiles para monitorizar el proceso de optimización, no reflejan directamente el desempeño final en la tarea de detección.\n"
     ]
    }
   ],
   "source": [
    "keywords = ['precision', 'recall', 'mAP50-95', 'mAP50']\n",
    "relevant1, optional, text_explanation = select_relevant_metrics(df,keywords)\n",
    "print(text_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han seleccionado las siguientes métricas principales para comparar el desempeño de detección:\n",
      "Velocidad CPU ONNX (ms), Parámetros (M), FLOPs (B).\n",
      "\n",
      "Estas métricas se consideran relevantes porque:\n",
      "- Las métricas con el prefijo 'metrics/' (por ejemplo, ) representan directamente la calidad de la detección (precisión, recall y mAP) en el conjunto de validación.\n",
      "- Los valores de 'mAP50' y 'mAP50-95' permiten evaluar el desempeño en diferentes umbrales de IoU, lo cual es crucial en tareas de detección.\n",
      "\n",
      "Adicionalmente, se consideran opcionales las siguientes columnas para analizar la eficiencia y convergencia:\n",
      "time, epoch.\n",
      "Estas columnas indican el tiempo de entrenamiento y el número de epochs, lo que ayuda a comparar la velocidad de convergencia y el costo computacional, aunque no son indicadores directos de la calidad en inferencia.\n",
      "\n",
      "Por otro lado, se omiten columnas con prefijos como 'train/' (pérdidas de entrenamiento) y 'lr/' (tasa de aprendizaje) ya que, aunque son útiles para monitorizar el proceso de optimización, no reflejan directamente el desempeño final en la tarea de detección.\n"
     ]
    }
   ],
   "source": [
    "keywords = ['Velocidad CPU ONNX (ms)','Parámetros (M)','FLOPs (B)']\n",
    "relevant2, optional, text_explanation = select_relevant_metrics(df,keywords)\n",
    "print(text_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>epoch</th>\n",
       "      <th>metrics/precision(B)</th>\n",
       "      <th>metrics/recall(B)</th>\n",
       "      <th>metrics/mAP50(B)</th>\n",
       "      <th>metrics/mAP50-95(B)</th>\n",
       "      <th>Velocidad CPU ONNX (ms)</th>\n",
       "      <th>Parámetros (M)</th>\n",
       "      <th>FLOPs (B)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.92370</td>\n",
       "      <td>0.93555</td>\n",
       "      <td>0.96995</td>\n",
       "      <td>0.91132</td>\n",
       "      <td>22.336352</td>\n",
       "      <td>2.584687</td>\n",
       "      <td>6393562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOLO12n</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.91198</td>\n",
       "      <td>0.95641</td>\n",
       "      <td>0.96774</td>\n",
       "      <td>0.91137</td>\n",
       "      <td>25.344527</td>\n",
       "      <td>2.559263</td>\n",
       "      <td>6409127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOLO11s</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.91603</td>\n",
       "      <td>0.92962</td>\n",
       "      <td>0.96505</td>\n",
       "      <td>0.90830</td>\n",
       "      <td>52.629673</td>\n",
       "      <td>9.417831</td>\n",
       "      <td>21457421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOLO12s</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.93749</td>\n",
       "      <td>0.92305</td>\n",
       "      <td>0.96780</td>\n",
       "      <td>0.90960</td>\n",
       "      <td>63.152516</td>\n",
       "      <td>9.235911</td>\n",
       "      <td>21383693600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Modelo  epoch  metrics/precision(B)  metrics/recall(B)  metrics/mAP50(B)  \\\n",
       "0  YOLO11n   50.0               0.92370            0.93555           0.96995   \n",
       "1  YOLO12n   50.0               0.91198            0.95641           0.96774   \n",
       "2  YOLO11s   50.0               0.91603            0.92962           0.96505   \n",
       "3  YOLO12s   50.0               0.93749            0.92305           0.96780   \n",
       "\n",
       "   metrics/mAP50-95(B)  Velocidad CPU ONNX (ms)  Parámetros (M)    FLOPs (B)  \n",
       "0              0.91132                22.336352        2.584687   6393562400  \n",
       "1              0.91137                25.344527        2.559263   6409127200  \n",
       "2              0.90830                52.629673        9.417831  21457421600  \n",
       "3              0.90960                63.152516        9.235911  21383693600  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[[\"Modelo\",\"epoch\"] + relevant1 + relevant2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este gráfico se comparan las siguientes métricas: metrics/precision(B), metrics/recall(B), metrics/mAP50(B), metrics/mAP50-95(B).\n",
      "Cada grupo de barras corresponde a un modelo, y cada color representa una métrica distinta.\n",
      "\n",
      "- Para la métrica 'metrics/precision(B)', el mejor modelo es **YOLO12s** con un valor de **0.9375**.\n",
      "- Para la métrica 'metrics/recall(B)', el mejor modelo es **YOLO12n** con un valor de **0.9564**.\n",
      "- Para la métrica 'metrics/mAP50(B)', el mejor modelo es **YOLO11n** con un valor de **0.9699**.\n",
      "- Para la métrica 'metrics/mAP50-95(B)', el mejor modelo es **YOLO12n** con un valor de **0.9114**.\n",
      "\n",
      "Resumen de cuántas veces cada modelo obtuvo el mejor valor:\n",
      "  - YOLO12n: 2 métrica(s)\n",
      "  - YOLO12s: 1 métrica(s)\n",
      "  - YOLO11n: 1 métrica(s)\n",
      "\n",
      "En base a estas métricas, el modelo con mayor cantidad de 'mejores resultados' se podría considerar superior. No obstante, la elección final puede depender de otras consideraciones (velocidad de inferencia, tamaño, etc.).\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_grouped_bars_and_explain(df, relevant_metrics, title=\"Comparación de métricas entre modelos\"):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras agrupadas para las métricas indicadas, comparando cada modelo.\n",
    "    Además, produce un texto explicativo indicando cuál modelo es mejor en cada métrica.\n",
    "\n",
    "    Parámetros:\n",
    "      - df (DataFrame): Debe contener una columna \"Modelo\" y columnas de métricas numéricas.\n",
    "      - relevant_metrics (list): Lista de nombres de columnas de métricas a graficar.\n",
    "      - title (str): Título opcional para el gráfico.\n",
    "\n",
    "    Retorna:\n",
    "      - explanation (str): Un texto que explica cuál modelo es mejor en cada métrica.\n",
    "    \"\"\"\n",
    "    # 1. Subconjunto con \"Modelo\" y las métricas relevantes\n",
    "    subset = df[[\"Modelo\"] + relevant_metrics].copy()\n",
    "\n",
    "    # 2. Establecer \"Modelo\" como índice para facilitar el gráfico\n",
    "    subset.set_index(\"Modelo\", inplace=True)\n",
    "\n",
    "    # 3. Crear gráfico de barras agrupadas\n",
    "    ax = subset.plot(kind=\"bar\", figsize=(10 + 1.5*len(relevant_metrics), 6), rot=0, title=title)\n",
    "    ax.set_ylabel(\"Valor de la métrica\")\n",
    "    # Mover la leyenda para que no tape el gráfico\n",
    "    plt.legend(title=\"Métricas\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Generar explicación sobre los resultados\n",
    "    explanation = f\"En este gráfico se comparan las siguientes métricas: {', '.join(relevant_metrics)}.\\n\"\n",
    "    explanation += \"Cada grupo de barras corresponde a un modelo, y cada color representa una métrica distinta.\\n\\n\"\n",
    "\n",
    "    # 5. Determinar el/los mejor(es) modelo(s) para cada métrica\n",
    "    best_models = {}\n",
    "    for metric in relevant_metrics:\n",
    "        # Buscar el valor máximo\n",
    "        max_value = subset[metric].max()\n",
    "        # Encontrar qué modelo(s) tienen ese valor máximo\n",
    "        best_mods = subset[subset[metric] == max_value].index.tolist()\n",
    "        best_models[metric] = (best_mods, max_value)\n",
    "\n",
    "    # 6. Construir texto explicativo de cuál es mejor en cada métrica\n",
    "    for metric, (best_mods, max_value) in best_models.items():\n",
    "        if len(best_mods) == 1:\n",
    "            explanation += f\"- Para la métrica '{metric}', el mejor modelo es **{best_mods[0]}** con un valor de **{max_value:.4f}**.\\n\"\n",
    "        else:\n",
    "            explanation += f\"- Para la métrica '{metric}', los mejores modelos son **{', '.join(best_mods)}** con un valor de **{max_value:.4f}**.\\n\"\n",
    "\n",
    "    # 7. Resumen: cuántas veces cada modelo fue el mejor\n",
    "    count_best = {}\n",
    "    for metric, (best_mods, _) in best_models.items():\n",
    "        for m in best_mods:\n",
    "            count_best[m] = count_best.get(m, 0) + 1\n",
    "\n",
    "    explanation += \"\\nResumen de cuántas veces cada modelo obtuvo el mejor valor:\\n\"\n",
    "    for m, c in sorted(count_best.items(), key=lambda x: x[1], reverse=True):\n",
    "        explanation += f\"  - {m}: {c} métrica(s)\\n\"\n",
    "\n",
    "    explanation += \"\\nEn base a estas métricas, el modelo con mayor cantidad de 'mejores resultados' se podría considerar superior. \" \\\n",
    "                   \"No obstante, la elección final puede depender de otras consideraciones (velocidad de inferencia, tamaño, etc.).\"\n",
    "\n",
    "    return explanation\n",
    "\n",
    "explanation_text = plot_grouped_bars_and_explain(df, relevant1)\n",
    "print(explanation_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_model_comparison(df, relevant_metrics):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras para cada métrica relevante comparando los modelos.\n",
    "    \n",
    "    Parámetros:\n",
    "      - df: DataFrame que contiene los resultados por modelo.\n",
    "      - relevant_metrics: Lista de columnas (métricas) a graficar.\n",
    "    \"\"\"\n",
    "    num_metrics = len(relevant_metrics)\n",
    "    # Si hay varias métricas, creamos subplots en una fila.\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(5*num_metrics, 5))\n",
    "    \n",
    "    # Si sólo hay una métrica, aseguramos que axes sea una lista.\n",
    "    if num_metrics == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for ax, metric in zip(axes, relevant_metrics):\n",
    "        # Verifica que la columna sea numérica para graficarla\n",
    "        if pd.api.types.is_numeric_dtype(df[metric]):\n",
    "            ax.bar(df[\"Modelo\"], df[metric], color='skyblue')\n",
    "            ax.set_title(metric)\n",
    "            ax.set_xlabel(\"Modelo\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f\"No es numérica: {metric}\", horizontalalignment='center', verticalalignment='center')\n",
    "            ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_model_comparison(df, relevant1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados\n",
    "\n",
    "1. **Métricas principales (precisión, recall, mAP50 y mAP50-95)**  \n",
    "   - **YOLO11s** alcanza la mayor precisión (\\(0.8074\\)), así como los mejores valores de mAP50 (\\(0.8679\\)) y mAP50-95 (\\(0.8167\\)). Esto indica que, en general, YOLO11s tiene la mayor capacidad para detectar correctamente los objetos (alta precisión) y un rendimiento sólido a diferentes umbrales de IoU (mAP).  \n",
    "   - **YOLO11n**, por otro lado, obtiene el mejor recall (\\(0.8399\\)). Esto significa que YOLO11n tiende a detectar un mayor porcentaje de objetos, aunque su precisión sea ligeramente menor.  \n",
    "   - **YOLO12n** y **YOLO12s** no superan a YOLO11s ni a YOLO11n en estas métricas específicas. YOLO12n tiene un recall un poco inferior a YOLO11n y una precisión ligeramente mayor que YOLO11n, pero no alcanza los valores de YOLO11s. Por su parte, YOLO12s queda algo rezagado en todas las métricas consideradas.\n",
    "\n",
    "2. **Tamaño del modelo (número de parámetros)**  \n",
    "   - **YOLO11s** y **YOLO12s** presentan aproximadamente 9.4M y 9.2M parámetros, respectivamente, mientras que **YOLO11n** y **YOLO12n** rondan los 2.5M. Esto implica que los modelos \"s\" (small) son significativamente más grandes que los \"n\" (nano) en términos de capacidad y, potencialmente, de coste computacional.  \n",
    "   - El hecho de que YOLO11s sea el modelo con mejor rendimiento coincide con que sea también uno de los más grandes, lo que sugiere que la mayor capacidad de parámetros podría estar aprovechándose para obtener un mejor ajuste a los datos.\n",
    "\n",
    "3. **Número de épocas (epoch)**  \n",
    "   - YOLO11s entrenó durante 42 épocas, mientras que YOLO12s solo 24. Esto puede indicar que YOLO12s no alcanzó su punto óptimo de entrenamiento. Del mismo modo, YOLO11n y YOLO12n entrenaron 27 y 33 épocas, respectivamente.  \n",
    "   - Podría ser interesante homogeneizar el número de épocas o aplicar técnicas de early stopping consistentes para comparar los modelos en igualdad de condiciones de entrenamiento.\n",
    "\n",
    "4. **Conclusión general**  \n",
    "   - **YOLO11s** es el modelo con mejor rendimiento global en las métricas de precisión, mAP50 y mAP50-95. Sin embargo, es también el más grande en número de parámetros (junto con YOLO12s), lo que implica mayor coste computacional en entrenamiento e inferencia.  \n",
    "   - **YOLO11n** destaca por su alto recall, lo que podría ser valioso en aplicaciones donde es preferible detectar tantos objetos como sea posible (aunque a costa de más falsos positivos). Además, YOLO11n es mucho más ligero que YOLO11s.  \n",
    "   - **YOLO12n** y **YOLO12s** no superan los resultados de sus contrapartes \"11\" en las métricas analizadas, si bien YOLO12n tiene un rendimiento cercano a YOLO11n y también un número de parámetros similar.  \n",
    "   - A la hora de elegir un modelo, es importante balancear la precisión, el recall y el mAP con la complejidad (número de parámetros) y la velocidad de inferencia. En este caso, si la prioridad absoluta es la calidad de detección, YOLO11s es el mejor de los cuatro; si la prioridad es un modelo ligero con alto recall, YOLO11n podría ser la mejor elección.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\D2_Fe-29-_jpg.rf.76c26d5b72254f1d6d3da88c0bbeadb7.jpg: 640x640 3 Falta-de-hierros, 13.7ms\n",
      "image 2/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\D2_Mn-38-_jpg.rf.55b0533d25249fcec8445f4b2c2f038f.jpg: 640x640 1 Falta-de-manganeso, 10.9ms\n",
      "image 3/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\D2_Mn-83-_jpg.rf.33c29492e9348d75d6606324350c5325.jpg: 640x640 1 Falta-de-hierro, 10.7ms\n",
      "image 4/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\Prueba escala alta.jpg: 448x640 1 Falta-de-boro, 4 Falta-de-calcios, 2 Falta-de-hierros, 15.9ms\n",
      "image 5/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\image prueba.jpg: 480x640 1 rust, 14.3ms\n",
      "image 6/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\imagen planta.jpeg: 480x640 1 healthy, 13.9ms\n",
      "image 7/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\imagen2.jpg: 384x640 1 healthy, 9.7ms\n",
      "image 8/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\miner.jpg: 448x640 1 healthy, 10.1ms\n",
      "image 9/9 C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\\nuevo.jpg: 640x640 1 healthy, 2 miners, 1 phoma, 1 rust, 11.6ms\n",
      "Speed: 2.1ms preprocess, 12.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\ALEX\\OneDrive\\Cursos\\Maestra en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\Datasets\\predicciones\\comb\\predicciones\u001b[0m\n",
      "Predicciones guardadas en: C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\Datasets\\predicciones\\comb/predicciones\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Cargar el modelo YOLO\n",
    "model_path = MODEL_DIR / 'fine_tuned' / 'YOLO11n.pt'\n",
    "modelo_pytorch = YOLO(model_path)  # Carga la arquitectura + pesos\n",
    "\n",
    "# Definir la carpeta de salida para predicciones\n",
    "pred_dir = OUTPUT_DIR / 'predictions'\n",
    "os.makedirs(pred_dir, exist_ok=True)  # Crear carpeta si no existe\n",
    "\n",
    "# Realizar predicciones y guardarlas en la carpeta de destino\n",
    "predictions = modelo_pytorch.predict(\n",
    "    source=r\"C:\\Users\\ALEX\\OneDrive\\Cursos\\Maestría en Big Data y Data Science\\Cursos-VIU\\Oblgatorios\\TFM\\detection-diseases-coffee\\imagenes de prueba\",\n",
    "    imgsz=640,\n",
    "    save=True,         # Guarda automáticamente las imágenes con anotaciones\n",
    "    project=pred_dir,  # Guarda los resultados en la carpeta personalizada\n",
    "    name=\"predicciones\"  # Nombre del subdirectorio dentro de `pred_dir`\n",
    ")\n",
    "\n",
    "print(f\"Predicciones guardadas en: {pred_dir}/predicciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación en tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\export_env\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta para el modelo ONNX\n",
    "onnx_path = MODEL_DIR / 'fine_tuned' / 'YOLO11n.onnx'\n",
    "\n",
    "# Cargar el modelo ONNX\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# Preparar la representación de TensorFlow\n",
    "tf_rep = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de clases en el modelo ONNX: 8400\n"
     ]
    }
   ],
   "source": [
    "# Obtener la información de los tensores de salida\n",
    "output_tensor = onnx_model.graph.output[0]\n",
    "output_shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]\n",
    "\n",
    "# La última dimensión suele ser el número de clases\n",
    "num_classes_onnx = output_shape[-1]\n",
    "\n",
    "print(f\"Cantidad de clases en el modelo ONNX: {num_classes_onnx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir_base = \"C://TEMP_MODEL_YOLO11n\" #  Ruta MUY corta y simple\n",
    "saved_model_dir = os.path.join(saved_model_dir_base, \"OUT\") # Subdirectorio también muy corto\n",
    "\n",
    "# Asegurarse de que el directorio base exista \n",
    "os.makedirs(saved_model_dir_base, exist_ok=True) # Asegura que el directorio \"Modelo\" existe\n",
    "# Asegurarse de que el directorio de salida del SavedModel exista\n",
    "os.makedirs(saved_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C://TEMP_MODEL_YOLO11n\\OUT/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C://TEMP_MODEL_YOLO11n\\OUT/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo convertido a TensorFlow SavedModel en: C://TEMP_MODEL_YOLO11n\\OUT\n"
     ]
    }
   ],
   "source": [
    "# Exportar el SavedModel\n",
    "tf_rep.export_graph(saved_model_dir)\n",
    "print(\"Modelo convertido a TensorFlow SavedModel en:\", saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firmas del modelo: ['serving_default']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo usando SavedModel API\n",
    "model = tf.saved_model.load(r\"C:\\TEMP_MODEL_YOLO11n\\OUT\")\n",
    "\n",
    "# Verificar las firmas del modelo\n",
    "print(\"Firmas del modelo:\", list(model.signatures.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de clases en el modelo SavedModel: 8400\n"
     ]
    }
   ],
   "source": [
    "# Obtener la salida del modelo desde la firma \"serving_default\"\n",
    "output_shape = list(model.signatures[\"serving_default\"].structured_outputs.values())[0].shape\n",
    "num_classes = output_shape[-1]  # Última dimensión representa las clases en clasificación\n",
    "\n",
    "print(f\"Cantidad de clases en el modelo SavedModel: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado a TFLite y guardado en: Modelo\\YOLO11n.tflite\n"
     ]
    }
   ],
   "source": [
    "# Convertir el SavedModel a TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Guardar el modelo TFLite\n",
    "tflite_path = MODEL_DIR / \"fine_tuned\" / \"YOLO11n.tflite\"\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"Modelo exportado a TFLite y guardado en:\", tflite_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
